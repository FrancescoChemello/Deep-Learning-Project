{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning**\n",
        "- **CNN**.\n",
        "- **RNN** implemented using a *LSTM*.\n",
        "\n",
        "It can uses also a *random seed* to shuffle the data and use a different *training*, *validation* and *test* set respect to the ones used by ***ViraMiner***."
      ],
      "metadata": {
        "id": "XesKIRwki-SK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kfMZfDCGW1B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main libraries\n",
        "\"\"\"\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For SMOTE\n",
        "\"\"\"\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "6zYXg7hrcbwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For the class weight\n",
        "\"\"\"\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "z-IIWBytvMzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For the ensamble learning\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "aTJgc4Axfk7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Only for the metrics analysis\n",
        "\"\"\"\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "MRQnBXcPjfBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7nDtGquLZ-",
        "outputId": "b6c94a76-5a11-4111-cbb9-1c1efbdf7896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Onehot Encoding** that transform a *DNA sequence* to a vector of zeros and ones. It is useful expectially for the **CNN**."
      ],
      "metadata": {
        "id": "V8Z62Tnwjifl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions for the onehot encoding\n",
        "\"\"\"\n",
        "\n",
        "def onehot_encoder(dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    \"\"\"\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    \"\"\"\n",
        "    Function that encodes a single DNA string into a onehot encoding string.\n",
        "    \"\"\"\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "\n",
        "    return encoder"
      ],
      "metadata": {
        "id": "UvkvwCS0jftS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_to_numeric(sequence):\n",
        "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "    return [mapping[nuc] for nuc in sequence]"
      ],
      "metadata": {
        "id": "89t-a71dbWnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CNN** model: direct from the file *CNN.ipynb* avaiable in the GitHub repository."
      ],
      "metadata": {
        "id": "9xFuLfEFkTJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!- CNN Model\n",
        "\"\"\"\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    # We can use a differnet pool for each layer\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(300, 200, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv1d(200, 100, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out_1 = nn.Dropout()\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv1d(100, 75, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(75),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv1d(75, 50, kernel_size = 2, padding = 1),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv1d(50, 32, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out_2 = nn.Dropout()\n",
        "\n",
        "        self.linear1 = nn.Linear(32, 128)\n",
        "        self.linear2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x=self.drop_out_1(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x=self.drop_out_2(x)\n",
        "        # Flatten the output for the linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "u-4GD7XkkRiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **RNN** model: direct from the file *LSTM.ipynb* avaiable in the GitHub repository. We choose the *LSTM* over the *GRU* because it obtained an **higher** precision on the dataset and also is **less** computationally demanding."
      ],
      "metadata": {
        "id": "Xd0t15Gjke_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Net (nn.Module):\n",
        "    def __init__ (self):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=6, batch_first=True, dropout=0.5, bidirectional=True)\n",
        "        # self.lstm = nn.LSTM(input_size=4, hidden_size=128, num_layers=2, dropout=0.5, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CFVonbFAkd7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN**"
      ],
      "metadata": {
        "id": "gOdTxRjflNbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training phase"
      ],
      "metadata": {
        "id": "uOcE4IfZlXCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction of the data from the *.cvc* files and **oversample** data from class 1."
      ],
      "metadata": {
        "id": "cGKQF_88kqIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!- MAIN\n",
        "\"\"\"\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "\"\"\"\n",
        "If runs in local machine:\n",
        "\n",
        "data_dir = os.path.abspath(os.path.join(os.getcwd(), 'data'))\n",
        "train_data_path = os.path.join(data_dir, 'fullset_train.csv')\n",
        "val_data_path = os.path.join(data_dir, 'fullset_validation.csv')\n",
        "test_data_path = os.path.join(data_dir, 'fullset_test.csv')\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "If runs in Google Drive\n",
        "\n",
        "rel_path_train = '/content/drive/MyDrive/Colab Notebooks/fullset_train.csv'\n",
        "rel_path_val = '/content/drive/MyDrive/Colab Notebooks/fullset_validation.csv'\n",
        "rel_path_test = '/content/drive/MyDrive/Colab Notebooks/fullset_test.csv'\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "If runs on Colab\n",
        "\n",
        "rel_path_train = '/content/fullset_train.csv'\n",
        "rel_path_val = '/content/fullset_validation.csv'\n",
        "rel_path_test = '/content/fullset_test.csv'\n",
        "\"\"\"\n",
        "\n",
        "# Paste here your path to the datasets\n",
        "rel_path_train = '/content/drive/MyDrive/Colab Notebooks/fullset_train.csv'\n",
        "rel_path_val = '/content/drive/MyDrive/Colab Notebooks/fullset_validation.csv'\n",
        "rel_path_test = '/content/drive/MyDrive/Colab Notebooks/fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "print(train_csv.describe())\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "\n",
        "print(\"Start SMOTE\")\n",
        "\n",
        "# Dataframe and upsample\n",
        "data = {'sequence' : train_data[:m,1],\n",
        "        'label' : train_data[:m,2].astype(np.int32) }\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X_train = df['sequence'].values\n",
        "Y_train = df['label'].values\n",
        "\n",
        "#Convert the sequences to numeric\n",
        "numeric_train = [sequence_to_numeric(seq) for seq in X_train]\n",
        "numeric_train = np.array(numeric_train)\n",
        "\n",
        "# Reshape the data\n",
        "n_samples, _ = numeric_train.shape\n",
        "numeric_train_sequences = numeric_train.reshape((n_samples, -1))\n",
        "\n",
        "# Apply the SMOTE algorithm to balance the dataset\n",
        "smote = SMOTE()\n",
        "X_train, Y_train = smote.fit_resample(numeric_train_sequences, Y_train)\n",
        "\n",
        "#Convert the sequences to string\n",
        "X_train = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_train]\n",
        "\n",
        "print(\"End of SMOTE\")\n",
        "\n",
        "\"\"\"\n",
        "Draft for oversample\n",
        "\n",
        "count_class_0, count_class_1 = df['label'].value_counts()\n",
        "\n",
        "df_class_0 = df[df['label'] == 0]\n",
        "df_class_1 = df[df['label'] == 1]\n",
        "\n",
        "# Oversample data from class 1 x10 -> ratio between class 1 and class 0 = 1/5\n",
        "n_samples = count_class_0 / 5\n",
        "\n",
        "print(\"Samples class 0: \", count_class_0)\n",
        "print(\"Samples class 1: \", count_class_1)\n",
        "print(\"Oversamples: \", int(n_samples))\n",
        "\n",
        "df_class_1_oversampled = df_class_1.sample(int(n_samples), replace=True, random_state=42)\n",
        "df_balanced = pd.concat([df_class_0, df_class_1_oversampled], axis=0)\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Extract data\n",
        "X_train = df_balanced['sequence'].values\n",
        "Y_train = df_balanced['label'].values\n",
        "\"\"\"\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train).to(device)\n",
        "Y_train = th.tensor(Y_train).to(device)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "print(val_csv.describe())\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val).to(device)\n",
        "Y_val = th.tensor(Y_val).to(device)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "print(test_csv.describe())\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test).to(device)\n",
        "Y_test = th.tensor(Y_test).to(device)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4TJX4a7QlZB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587b0e44-010c-46d3-b7fe-898b73961f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n",
            "                   0\n",
            "count  211238.000000\n",
            "mean        0.021142\n",
            "std         0.143858\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         0.000000\n",
            "max         1.000000\n",
            "Start SMOTE\n",
            "End of SMOTE\n",
            "Start onehot encoding for the training data\n",
            "X_train shape:  torch.Size([413544, 300, 4])\n",
            "Y_train shape:  torch.Size([413544])\n",
            "                  0\n",
            "count  26404.000000\n",
            "mean       0.020224\n",
            "std        0.140769\n",
            "min        0.000000\n",
            "25%        0.000000\n",
            "50%        0.000000\n",
            "75%        0.000000\n",
            "max        1.000000\n",
            "Start onehot encoding for the validation data\n",
            "X_val shape torch.Size([26404, 300, 4])\n",
            "Y_val shape torch.Size([26404])\n",
            "                  0\n",
            "count  26404.000000\n",
            "mean       0.020868\n",
            "std        0.142945\n",
            "min        0.000000\n",
            "25%        0.000000\n",
            "50%        0.000000\n",
            "75%        0.000000\n",
            "max        1.000000\n",
            "Start onehot encoding for the test data\n",
            "X_test shape torch.Size([26404, 300, 4])\n",
            "Y_test shape torch.Size([26404])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffle of the data** *(optional)* using a random seed and split data:\n",
        "- **Train Set:** around the 80% of the total samples.\n",
        "- **Validation Set:** around 10% of the total samples.\n",
        "- **Test Set:** around 10% of the total samples."
      ],
      "metadata": {
        "id": "pyFnowCTl6p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I merge the three tensor array to a big one and then, after a shuffle, I split the data into:\n",
        "  - Training: 248126 data\n",
        "  - Validation: 31034 data\n",
        "  - Test: 31033 data\n",
        "\n",
        "  Random seed: 42 (the most used)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "If we use SMOTE we do not use random seed!!\n",
        "\n",
        "\n",
        "# Merge the arrays\n",
        "\n",
        "print(\"Shape of X_train: \", X_train.shape)\n",
        "print(\"Shape of X_val: \", X_val.shape)\n",
        "print(\"Shape of X_test: \", X_test.shape)\n",
        "\n",
        "X_data = th.concat((X_train, X_val), axis = 0).to(device)\n",
        "X_data = th.concat((X_data, X_test), axis = 0).to(device)\n",
        "\n",
        "print(\"Shape of data: \", X_data.shape)\n",
        "\n",
        "# Free memory\n",
        "del X_train, X_val, X_test\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "print(\"Shape of Y_train: \", Y_train.shape)\n",
        "print(\"Shape of Y_val: \", Y_val.shape)\n",
        "print(\"Shape of Y_test: \", Y_test.shape)\n",
        "\n",
        "Y_data = th.concat((Y_train, Y_val), axis = 0).to(device)\n",
        "Y_data = th.concat((Y_data, Y_test), axis = 0).to(device)\n",
        "\n",
        "print(\"Shape of data: \", Y_data.shape)\n",
        "\n",
        "# Class weight\n",
        "classes_unique = np.unique(Y_data.cpu())\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes_unique, y=[0,1])\n",
        "class_weights = th.tensor(class_weights, dtype=th.float32)\n",
        "\n",
        "# Free memory\n",
        "del Y_train, Y_val, Y_test\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "# Random Seed\n",
        "random_seed = 42\n",
        "th.manual_seed(random_seed)\n",
        "\n",
        "X_index_shuffle = th.randperm(X_data.size(0))\n",
        "X_data_shuffled = X_data[X_index_shuffle]\n",
        "\n",
        "Y_index_shuffle = th.randperm(Y_data.size(0))\n",
        "Y_data_shuffled = Y_data[Y_index_shuffle]\n",
        "\n",
        "print(\"X_data_shuffled shape: \", X_data_shuffled.shape)\n",
        "print(\"Y_data_shuffled shape: \", Y_data_shuffled.shape)\n",
        "\n",
        "# Split data into Training, Validation and Test\n",
        "X_train = X_data_shuffled[148126:248126].cpu()\n",
        "Y_train = Y_data_shuffled[148126:248126].cpu()\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "X_val = X_data_shuffled[211238:279160].cpu()\n",
        "Y_val = Y_data_shuffled[211238:279160].cpu()\n",
        "print(\"X_val shape: \", X_val.shape)\n",
        "print(\"Y_val shape: \", Y_val.shape)\n",
        "\n",
        "X_test = X_data_shuffled[279160:].cpu()\n",
        "Y_test = Y_data_shuffled[279160:].cpu()\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"Y_test shape: \", Y_test.shape)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j2XRhmhll3qP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "20b60d57-bf40-4209-92de-94322a425732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIf we use SMOTE we do not use random seed!!\\n\\n\\n# Merge the arrays\\n\\nprint(\"Shape of X_train: \", X_train.shape)\\nprint(\"Shape of X_val: \", X_val.shape)\\nprint(\"Shape of X_test: \", X_test.shape)\\n\\nX_data = th.concat((X_train, X_val), axis = 0).to(device)\\nX_data = th.concat((X_data, X_test), axis = 0).to(device)\\n\\nprint(\"Shape of data: \", X_data.shape)\\n\\n# Free memory\\ndel X_train, X_val, X_test\\ngc.collect()\\nth.cuda.empty_cache()\\n\\nprint(\"Shape of Y_train: \", Y_train.shape)\\nprint(\"Shape of Y_val: \", Y_val.shape)\\nprint(\"Shape of Y_test: \", Y_test.shape)\\n\\nY_data = th.concat((Y_train, Y_val), axis = 0).to(device)\\nY_data = th.concat((Y_data, Y_test), axis = 0).to(device)\\n\\nprint(\"Shape of data: \", Y_data.shape)\\n\\n# Class weight\\nclasses_unique = np.unique(Y_data.cpu())\\nclass_weights = compute_class_weight(class_weight=\\'balanced\\', classes=classes_unique, y=[0,1])\\nclass_weights = th.tensor(class_weights, dtype=th.float32)\\n\\n# Free memory\\ndel Y_train, Y_val, Y_test\\ngc.collect()\\nth.cuda.empty_cache()\\n\\n# Random Seed\\nrandom_seed = 42\\nth.manual_seed(random_seed)\\n\\nX_index_shuffle = th.randperm(X_data.size(0))\\nX_data_shuffled = X_data[X_index_shuffle]\\n\\nY_index_shuffle = th.randperm(Y_data.size(0))\\nY_data_shuffled = Y_data[Y_index_shuffle]\\n\\nprint(\"X_data_shuffled shape: \", X_data_shuffled.shape)\\nprint(\"Y_data_shuffled shape: \", Y_data_shuffled.shape)\\n\\n# Split data into Training, Validation and Test\\nX_train = X_data_shuffled[148126:248126].cpu()\\nY_train = Y_data_shuffled[148126:248126].cpu()\\nprint(\"X_train shape: \", X_train.shape)\\nprint(\"Y_train shape: \", Y_train.shape)\\n\\nX_val = X_data_shuffled[211238:279160].cpu()\\nY_val = Y_data_shuffled[211238:279160].cpu()\\nprint(\"X_val shape: \", X_val.shape)\\nprint(\"Y_val shape: \", Y_val.shape)\\n\\nX_test = X_data_shuffled[279160:].cpu()\\nY_test = Y_data_shuffled[279160:].cpu()\\nprint(\"X_test shape: \", X_test.shape)\\nprint(\"Y_test shape: \", Y_test.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight class**"
      ],
      "metadata": {
        "id": "hFyToHmlweav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to calculate the weight for the two classes for\n",
        "\"\"\"\n",
        "def get_class_weights(dataset):\n",
        "    labels = [label for label in dataset]\n",
        "    class_counts = np.bincount(labels)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "    return th.tensor(class_weights, dtype=th.float)\n",
        "\n",
        "# Calculate the class weights for the X_train\n",
        "class_weights = get_class_weights(Y_train.cpu()).to(device)\n",
        "\n",
        "print(\"Class weight: \", class_weights)"
      ],
      "metadata": {
        "id": "ceX61kVJweAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9835a06f-a4cb-4e81-81a3-d6adf6f113ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weight:  tensor([1., 1.], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Class to create a Data Loader with X label and Y label together\n",
        "\"\"\"\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_point = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return data_point, label\n",
        "\n",
        "# Create the Dataset\n",
        "train_dataset = CreateDataset(X_train, Y_train)\n",
        "val_dataset = CreateDataset(X_val, Y_val)\n",
        "test_dataset = CreateDataset(X_test, Y_test)\n",
        "\n",
        "# Batch size\n",
        "batch_dim = 128\n",
        "\n",
        "# Create the Data Loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_dim, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=True)\n",
        "\n",
        "# Free memory\n",
        "del X_train, X_val, Y_val\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nfGNRl3pxyO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train* of the **CNN** using the *Validation* set to check the performance."
      ],
      "metadata": {
        "id": "YsILBtnjmSD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the CNN\")\n",
        "\n",
        "model_CNN = ConvNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = th.optim.AdamW(model_CNN.parameters(), lr=0.001)\n",
        "scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5  # CHECK THE VALUE!!\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_CNN.train()\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        X_batch = X_batch.float()\n",
        "        Y_batch = Y_batch.long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_CNN(X_batch.to(device))\n",
        "        loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "    # Validation the model\n",
        "    model_CNN.eval()\n",
        "    val_loss = 0.0\n",
        "    j = 0\n",
        "    with th.no_grad():\n",
        "        for X_batch, Y_batch in val_loader:\n",
        "            X_batch = X_batch.float()\n",
        "            Y_batch = Y_batch.long()\n",
        "\n",
        "            outputs = model_CNN(X_batch.to(device))\n",
        "            loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            j += 1\n",
        "\n",
        "            # Free memory\n",
        "            del X_batch, Y_batch\n",
        "            th.cuda.empty_cache()\n",
        "\n",
        "    # Losses\n",
        "    running_loss = running_loss/i\n",
        "    val_loss = val_loss/j\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss <= best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"End training the CNN\")\n",
        "\n",
        "# Free memory\n",
        "del i, j, running_loss, val_loss\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7SC620P4mZ92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201142b7-5049-4328-bc13-f9136804fcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the CNN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.1268, Val Loss: 0.1121, Patience: 5\n",
            "Epoch [2/100], Train Loss: 0.0925, Val Loss: 0.1121, Patience: 4\n",
            "Epoch [3/100], Train Loss: 0.0763, Val Loss: 0.1171, Patience: 3\n",
            "Epoch [4/100], Train Loss: 0.0678, Val Loss: 0.1077, Patience: 5\n",
            "Epoch [5/100], Train Loss: 0.0631, Val Loss: 0.1074, Patience: 5\n",
            "Epoch [6/100], Train Loss: 0.0603, Val Loss: 0.1061, Patience: 5\n",
            "Epoch [7/100], Train Loss: 0.0584, Val Loss: 0.1115, Patience: 4\n",
            "Epoch [8/100], Train Loss: 0.0569, Val Loss: 0.1112, Patience: 3\n",
            "Epoch [9/100], Train Loss: 0.0556, Val Loss: 0.1133, Patience: 2\n",
            "Epoch [10/100], Train Loss: 0.0545, Val Loss: 0.1094, Patience: 1\n",
            "Epoch [11/100], Train Loss: 0.0501, Val Loss: 0.1114, Patience: 0\n",
            "Early stopping at epoch 11\n",
            "End training the CNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(OPTIONAL):** *Test* results for the **CNN**."
      ],
      "metadata": {
        "id": "aL4g50f6ocxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the CNN\")\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_CNN.eval()\n",
        "\n",
        "# Arrays to save the results\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    for X_batch, Y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        outputs = model_CNN(X_batch)\n",
        "        _, predicted = th.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(Y_batch.cpu().numpy())\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true_CNN = all_labels\n",
        "y_pred_CNN = all_preds\n",
        "\n",
        "# Metrics\n",
        "accuracy_CNN = accuracy_score(y_true_CNN, y_pred_CNN)\n",
        "precision_CNN = precision_score(y_true_CNN, y_pred_CNN, average='weighted', zero_division=0)\n",
        "recall_CNN = recall_score(y_true_CNN, y_pred_CNN, average='weighted')\n",
        "f1_CNN = f1_score(y_true_CNN, y_pred_CNN, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy_CNN:.6f}')\n",
        "print(f'Precision: {precision_CNN:.6f}')\n",
        "print(f'Recall: {recall_CNN:.6f}')\n",
        "print(f'F1-score: {f1_CNN:.6f}')\n",
        "\n",
        "print(\"End testing the CNN\")\n",
        "\n",
        "# Free memory\n",
        "del all_labels, all_preds, y_true_CNN, y_pred_CNN, accuracy_CNN, precision_CNN, recall_CNN, f1_CNN\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CBBUkJquokqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b469d8d8-02f9-4c68-e018-d38b1573f155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start testing the CNN\n",
            "Accuracy: 0.977200\n",
            "Precision: 0.962646\n",
            "Recall: 0.977200\n",
            "F1-score: 0.968700\n",
            "End testing the CNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train* of the **RNN** using the *Validation* set to check the performance."
      ],
      "metadata": {
        "id": "1G5i7_hGmvV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the LSTM\")\n",
        "\n",
        "# Model, loss function and optimizer\n",
        "model_LSTM = LSTM_Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = th.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5  # CHECK THE VALUE!!\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_LSTM.train()\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_LSTM(X_batch.to(device))\n",
        "        loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "    # Validation\n",
        "    model_LSTM.eval()\n",
        "    val_loss = 0.0\n",
        "    j = 0\n",
        "    with th.no_grad():\n",
        "        for X_batch, Y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device).float()\n",
        "            Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "            outputs = model_LSTM(X_batch)\n",
        "            loss = criterion(outputs, Y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            j += 1\n",
        "\n",
        "            # Free memory\n",
        "            del X_batch, Y_batch\n",
        "            th.cuda.empty_cache()\n",
        "\n",
        "    # Losses\n",
        "    running_loss = running_loss/i\n",
        "    val_loss = val_loss/j\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss <= best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"End training the LSTM\")\n",
        "\n",
        "# Free memory\n",
        "del i, j, running_loss, val_loss\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8ePulED-mxXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa65f59-fe3e-4ed8-a97d-c57ee0a3f1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training the LSTM\n",
            "Epoch [1/100], Train Loss: 0.2236, Val Loss: 0.1288, Patience: 5\n",
            "Epoch [2/100], Train Loss: 0.1167, Val Loss: 0.1556, Patience: 4\n",
            "Epoch [3/100], Train Loss: 0.1070, Val Loss: 0.1266, Patience: 5\n",
            "Epoch [4/100], Train Loss: 0.0949, Val Loss: 0.1039, Patience: 5\n",
            "Epoch [5/100], Train Loss: 0.0797, Val Loss: 0.1483, Patience: 4\n",
            "Epoch [6/100], Train Loss: 0.0699, Val Loss: 0.1062, Patience: 3\n",
            "Epoch [7/100], Train Loss: 0.0634, Val Loss: 0.0998, Patience: 5\n",
            "Epoch [8/100], Train Loss: 0.0593, Val Loss: 0.1042, Patience: 4\n",
            "Epoch [9/100], Train Loss: 0.0562, Val Loss: 0.0976, Patience: 5\n",
            "Epoch [10/100], Train Loss: 0.0534, Val Loss: 0.1039, Patience: 4\n",
            "Epoch [11/100], Train Loss: 0.0534, Val Loss: 0.0941, Patience: 5\n",
            "Epoch [12/100], Train Loss: 0.0486, Val Loss: 0.0953, Patience: 4\n",
            "Epoch [13/100], Train Loss: 0.0472, Val Loss: 0.0917, Patience: 5\n",
            "Epoch [14/100], Train Loss: 0.0460, Val Loss: 0.0912, Patience: 5\n",
            "Epoch [15/100], Train Loss: 0.0439, Val Loss: 0.1026, Patience: 4\n",
            "Epoch [16/100], Train Loss: 0.0430, Val Loss: 0.0937, Patience: 3\n",
            "Epoch [17/100], Train Loss: 0.0406, Val Loss: 0.0978, Patience: 2\n",
            "Epoch [18/100], Train Loss: 0.0393, Val Loss: 0.0971, Patience: 1\n",
            "Epoch [19/100], Train Loss: 0.0383, Val Loss: 0.1025, Patience: 0\n",
            "Early stopping at epoch 19\n",
            "End training the LSTM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(OPTIONAL):** *Test* results forthe **RNN**."
      ],
      "metadata": {
        "id": "RCAvQnFzotd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the LSTM\")\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_LSTM.eval()\n",
        "\n",
        "# Arrays to save the results\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    for X_batch, Y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        outputs = model_LSTM(X_batch)\n",
        "        _, predicted = th.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(Y_batch.cpu().numpy())\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true_LSTM = all_labels\n",
        "y_pred_LSTM = all_preds\n",
        "\n",
        "# Metrics\n",
        "accuracy_LSTM = accuracy_score(y_true_LSTM, y_pred_LSTM)\n",
        "precision_LSTM = precision_score(y_true_LSTM, y_pred_LSTM, average='weighted', zero_division=0)\n",
        "recall_LSTM = recall_score(y_true_LSTM, y_pred_LSTM, average='weighted')\n",
        "f1_LSTM = f1_score(y_true_LSTM, y_pred_LSTM, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy_LSTM:.6f}')\n",
        "print(f'Precision: {precision_LSTM:.6f}')\n",
        "print(f'Recall: {recall_LSTM:.6f}')\n",
        "print(f'F1-score: {f1_LSTM:.6f}')\n",
        "\n",
        "print(\"End testing the LSTM\")\n",
        "\n",
        "# Free memory\n",
        "del all_labels, all_preds, y_true_LSTM, y_pred_LSTM, accuracy_LSTM, precision_LSTM, recall_LSTM, f1_LSTM\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Fc4lNi6IoqON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74d186a-3ca1-4d77-c5cb-40f7e3282dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start testing the LSTM\n",
            "Accuracy: 0.973981\n",
            "Precision: 0.965949\n",
            "Recall: 0.973981\n",
            "F1-score: 0.969472\n",
            "End testing the LSTM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of the **Ensemble Learning** using an **SVM** (stacking ensemble learing)."
      ],
      "metadata": {
        "id": "oOhwQoMemxyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An SVM takes in input the results from the CNN and RNN and then it preforms the final classification.\n",
        "\n",
        "I have to use skitlearn to do the ensable learing because the library is only avaiable on it!\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Function that, given a model and a data loader, it returns all the predictions (outputs)\n",
        "\"\"\"\n",
        "def get_predictions(model_pred, data_loader):\n",
        "    model_pred.eval()\n",
        "    all_preds = []\n",
        "    with th.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            outputs = model_pred(inputs.float().to(device))\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "    return np.concatenate(all_preds)\n",
        "\n",
        "# Prepare the prediction for the training model\n",
        "CNN_train_preds = get_predictions(model_CNN, train_loader)\n",
        "LSTM_train_preds = get_predictions(model_LSTM, train_loader)\n",
        "\n",
        "# Prepare the prediction for the testing model\n",
        "CNN_test_preds = get_predictions(model_CNN, test_loader)\n",
        "LSTM_test_preds = get_predictions(model_LSTM, test_loader)\n",
        "\n",
        "# Concatenate the two inputs\n",
        "X_train_meta = np.hstack((CNN_train_preds, LSTM_train_preds))\n",
        "\n",
        "print(\"Shape of X_train_meta: \", X_train_meta.shape)\n",
        "print(\"Shape of Y_train: \", Y_train.shape)\n",
        "\n",
        "# Define the train loader\n",
        "train_dataset = CreateDataset(X_train_meta, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
        "\n",
        "# Define the test loader\n",
        "test_dataset = CreateDataset(X_test, Y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=True)\n",
        "\n",
        "# Free memory\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5PlI9nhBiojA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69ee40a-5231-4a63-b43e-c7e3647006d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_meta:  (413544, 4)\n",
            "Shape of Y_train:  torch.Size([413544])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use a polynomial SVM for the final classification\n",
        "model_SVM = SVC(kernel = 'poly', probability = True)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_SVM.fit(X_batch, Y_batch.ravel())"
      ],
      "metadata": {
        "id": "f0X4pYiTm7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test** of the performance."
      ],
      "metadata": {
        "id": "PaBpALEtmo0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the final model - SVM\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_prob = []\n",
        "\n",
        "# Testing the meta-model: SVM\n",
        "# y_pred = model_SVM.predict(X_test.cpu())\n",
        "# y_pred_prob = model_SVM.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# model_SVM.eval()\n",
        "with th.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.cpu()\n",
        "        labels = labels.cpu().numpy()\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        inputs_np = inputs.numpy() if isinstance(inputs, th.Tensor) else inputs\n",
        "        current_batch_size = inputs_np.shape[0]\n",
        "\n",
        "        batch_pred = model_SVM.predict(inputs_np)\n",
        "        batch_pred_prob = model_SVM.predict_proba(inputs_np)[:, 1]\n",
        "\n",
        "        y_pred.extend(batch_pred.tolist())\n",
        "        y_pred_prob.extend(batch_pred_prob.tolist())\n",
        "\n",
        "# Convert y to numpy array\n",
        "y_true = np.array(Y_test.cpu())\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_prob = np.array(y_pred_prob)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - SVM\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "c8YnAeVfmlAC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "c96519b5-07f1-4a3f-f381-b05b967b6d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start testing the final model - SVM\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with dim 3. SVC expected <= 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-405b098d66b3>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcurrent_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbatch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_SVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mbatch_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_SVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             X = self._validate_data(\n\u001b[0m\u001b[1;32m    614\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    913\u001b[0m             )\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    916\u001b[0m                 \u001b[0;34m\"Found array with dim %d. %s expected <= 2.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. SVC expected <= 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "FcYdtZFbqXfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_XGB = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_XGB.fit(X_batch, Y_batch.ravel())"
      ],
      "metadata": {
        "id": "dGnt397eqblj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the final model - XGB\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_prob = []\n",
        "\n",
        "# Testing the meta-model: XGB\n",
        "# y_pred = model_XGB.predict(X_test.cpu())\n",
        "# y_pred_prob = model_XGB.predict_proba(X_test)[:, 1]\n",
        "\n",
        "model_XGB.eval()\n",
        "with th.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.cpu()\n",
        "        labels = labels.cpu().numpy()\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        inputs_np = inputs.numpy() if isinstance(inputs, th.Tensor) else inputs\n",
        "        current_batch_size = inputs_np.shape[0]\n",
        "\n",
        "        batch_pred = model_XGB.predict(inputs_np)\n",
        "        batch_pred_prob = model_XGB.predict_proba(inputs_np)[:, 1]\n",
        "\n",
        "        y_pred.extend(batch_pred.tolist())\n",
        "        y_pred_prob.extend(batch_pred_prob.tolist())\n",
        "\n",
        "# Convert y to numpy array\n",
        "y_true = np.array(Y_test.cpu())\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_prob = np.array(y_pred_prob)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - XGB\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "x2F-yEtC6LdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "gofpkuLioI4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_RF.fit(X_batch, Y_batch.ravel())"
      ],
      "metadata": {
        "id": "8ZnTiexFoJ6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the final model - RF\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_prob = []\n",
        "\n",
        "# Testing the meta-model: RF\n",
        "# y_pred = model_RF.predict(X_test.cpu())\n",
        "# y_pred_prob = model_RF.predict_proba(X_test)[:, 1]\n",
        "\n",
        "model_RF.eval()\n",
        "with th.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.cpu()\n",
        "        labels = labels.cpu().numpy()\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        inputs_np = inputs.numpy() if isinstance(inputs, th.Tensor) else inputs\n",
        "        current_batch_size = inputs_np.shape[0]\n",
        "\n",
        "        batch_pred = model_RF.predict(inputs_np)\n",
        "        batch_pred_prob = model_RF.predict_proba(inputs_np)[:, 1]\n",
        "\n",
        "        y_pred.extend(batch_pred.tolist())\n",
        "        y_pred_prob.extend(batch_pred_prob.tolist())\n",
        "\n",
        "# Convert y to numpy array\n",
        "y_true = np.array(Y_test.cpu())\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_prob = np.array(y_pred_prob)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - RF\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_STpWxhUoh2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "-WI9X35eqFNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_GBC.fit(X_batch, Y_batch.ravel())"
      ],
      "metadata": {
        "id": "Dwz10Gf4qGOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the final model - GBC\")\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_prob = []\n",
        "\n",
        "# Testing the meta-model: RF\n",
        "# y_pred = model_GBC.predict(X_test.cpu())\n",
        "# y_pred_prob = model_GBC.predict_proba(X_test)[:, 1]\n",
        "\n",
        "model_GBC.eval()\n",
        "with th.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.cpu()\n",
        "        labels = labels.cpu().numpy()\n",
        "        y_true.extend(labels.tolist())\n",
        "\n",
        "        inputs_np = inputs.numpy() if isinstance(inputs, th.Tensor) else inputs\n",
        "        current_batch_size = inputs_np.shape[0]\n",
        "\n",
        "        batch_pred = model_GBC.predict(inputs_np)\n",
        "        batch_pred_prob = model_GBC.predict_proba(inputs_np)[:, 1]\n",
        "\n",
        "        y_pred.extend(batch_pred.tolist())\n",
        "        y_pred_prob.extend(batch_pred_prob.tolist())\n",
        "\n",
        "# Convert y to numpy array\n",
        "y_true = np.array(Y_test.cpu())\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_prob = np.array(y_pred_prob)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - GBC\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bhNFT4ThqWAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}