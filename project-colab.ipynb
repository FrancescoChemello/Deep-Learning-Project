{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEXQ68PmzbC8SZgGlrai9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescoChemello/Deep-Learning-Project/blob/main/project-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning project aa 2023/24"
      ],
      "metadata": {
        "id": "-lP0Ty_LPOPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "RXCvJMNiPWPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czu9e1vBNszI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# For the progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that perform the onehot encoding"
      ],
      "metadata": {
        "id": "8qIn2_aoP_Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_encoder (dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    The dataset is a list of strings, each string is a DNA sequence.\n",
        "    The function returns a list of lists, each list is a onehot encoding of a DNA sequence.\n",
        "    \"\"\"\n",
        "    # Define the dictionary for the onehot encoding\n",
        "    onehot_dict = {'A':[1,0,0,0], 'C':[0,1,0,0], 'G':[0,0,1,0], 'T':[0,0,0,1]}\n",
        "    # Define the chunk size for the export of the data to a numpy array\n",
        "    chunk_size = 10000\n",
        "    # Initialize the onehot dataset\n",
        "    onehot_dataset = []\n",
        "    # Iterate over the dataset\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    for sequence in dataset:\n",
        "        # Initialize the onehot sequence\n",
        "        onehot_sequence = np.array([])\n",
        "        # Iterate over the sequence\n",
        "        for base in sequence:\n",
        "            # Append the onehot base\n",
        "            onehot_sequence = np.append(onehot_sequence, onehot_dict[base])\n",
        "        # Append the onehot sequence\n",
        "        onehot_dataset.append(onehot_sequence)\n",
        "        pbar.update(1)\n",
        "    # Convert the onehot dataset to a numpy array to have an arry of arrays\n",
        "    pbar.close()\n",
        "    print(\"Starting convertion to numpy array\")\n",
        "    # Convert the onehot dataset to a numpy array in chunks to avoid memory issues\n",
        "    onehot_dataset_numpy = np.empty([0, 1200])\n",
        "    for i in range(0, len(onehot_dataset), chunk_size):\n",
        "        if i+chunk_size < len(onehot_dataset):\n",
        "            onehot_dataset_numpy = np.concatenate((onehot_dataset_numpy, np.asarray(onehot_dataset[i:i+chunk_size])))\n",
        "        else:\n",
        "           onehot_dataset_numpy = np.concatenate((onehot_dataset_numpy, np.asarray(onehot_dataset[i:])))\n",
        "    print(\"Type of onehot_dataset_numpy: \", type(onehot_dataset_numpy))\n",
        "    print(\"Shape of onehot_dataset_numpy: \", onehot_dataset_numpy.shape)\n",
        "    print(\"Onehot encoding done\")\n",
        "    return onehot_dataset"
      ],
      "metadata": {
        "id": "Jbw00zSUP9u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding"
      ],
      "metadata": {
        "id": "CQstKCYgQSfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!- MAIN\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "absolute_path = os.path.dirname(__file__)\n",
        "rel_path_train = 'data\\\\fullset_train.csv'\n",
        "rel_path_val = 'data\\\\fullset_validation.csv'\n",
        "rel_path_test = 'data\\\\fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(os.path.join(absolute_path, rel_path_train), sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "train_csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_train = train_data[:m,1]\n",
        "Y_train = train_data[:m,2].astype(np.int32)\n",
        "\n",
        "# Reduce the size of the dataset for testing\n",
        "m = 100000\n",
        "X_train = X_train[:m]\n",
        "Y_train = Y_train[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.tensor(X_train)\n",
        "Y_train = th.tensor(Y_train)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "\n",
        "exit() # Debug -END OF ONEHOT ENCODING-\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(os.path.join(absolute_path, rel_path_val), sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "val_csv.describe()\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2]\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(np.array(X_val))\n",
        "Y_val = th.from_numpy(np.array(Y_val))\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(os.path.join(absolute_path, rel_path_test), sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "test_csv.describe()\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2]\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(np.array(X_test))\n",
        "Y_test = th.from_numpy(np.array(Y_test))\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m\n",
        "\n",
        "exit() # Debug -END OF READING DATA-S"
      ],
      "metadata": {
        "id": "7gYkIfnYQRGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "HSnStN2NQbRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the model\")\n",
        "\n",
        "# RNN\n",
        "\n",
        "# 1. Define the model\n",
        "# Hyperparameters for the model (need a vector in a k-fold validation):\n",
        "#   input_size – The number of expected features in the input x\n",
        "#   hidden_size – The number of features in the hidden state h\n",
        "#   num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
        "#   bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "#   batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
        "#   dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "#   bidirectional – If True, becomes a bidirectional GRU. Default:\n",
        "\n",
        "input_size = 4  # A, C, G, T\n",
        "hidden_size = 64  # To be defined\n",
        "output_size = 2  # We want a probabilistic value\n",
        "num_layers = 1  # Just one GRU and not n GRU stacked\n",
        "bias = True # We want to use bias\n",
        "batch_first = False # The input and output tensors are provided as (seq, batch, feature)\n",
        "dropout = 0.0 # No dropout\n",
        "bidirectional = True # We want a bidirectional GRU\n",
        "\n",
        "# INPUT: input, h_0\n",
        "# -input: tensor shape (L, H_in) where L is the sequence length and H_in is the input size.\n",
        "# -h_0: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "# OUTPUT: output, h_n\n",
        "# -output: tensor shape (L, D*H_out) where L is the sequence length, D is 2 for bidirectional and H_out is the hidden size.\n",
        "# -h_n: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "\n",
        "model = th.nn.GRU(input_size, hidden_size, num_layers, num_classes, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, device=None, dtype=None).to(device)\n",
        "criterion = nn.CrossEntropyLoss()   #the most common loss function used for classification problems\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 2. Train the model with epochs\n",
        "# for epoch in range(num_epochs):\n",
        "#     # TODO: Implement the training loop\n",
        "#     break\n",
        "\n",
        "model.train()\n",
        "# 3. Test the model using epochs\n",
        "\n",
        "print(\"End of training the model\")\n",
        "\n",
        "print(\"Start testing the model\")\n",
        "\n",
        "# 4. Evaluate the model\n",
        "predicted_values, _ = model(test_data)\n",
        "\n",
        "print(\"End of testing the model\")"
      ],
      "metadata": {
        "id": "ENKk0h2VQZiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}