{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main libraries\n",
    "\"\"\"\n",
    "import os\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for the onehot encoding\n",
    "\"\"\"\n",
    "\n",
    "def onehot_encoder(dataset):\n",
    "    \"\"\"\n",
    "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
    "    \"\"\"\n",
    "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
    "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
    "\n",
    "    return onehot_dataset_numpy\n",
    "\n",
    "\n",
    "def dna_onehot_encoder(dna_sequence):\n",
    "    \"\"\"\n",
    "    Function that encodes a single DNA string into a onehot encoding string.\n",
    "    \"\"\"\n",
    "    onehot_dict = {\n",
    "        'A' : [1, 0, 0, 0],\n",
    "        'C' : [0, 1, 0, 0],\n",
    "        'G' : [0, 0, 1, 0],\n",
    "        'T' : [0, 0, 0, 1]\n",
    "    }\n",
    "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Net (nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=6, batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        # self.lstm = nn.LSTM(input_size=4, hidden_size=128, num_layers=2, dropout=0.5, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_numeric(sequence):\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    return [mapping[nuc] for nuc in sequence]\n",
    "\n",
    "# def numeric_to_sequence(numeric_sequence):\n",
    "#     mapping = ['A', 'C', 'G', 'T']\n",
    "#     return ''.join([mapping[nuc] for nuc in numeric_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Start one hot encoding for training\n",
      "End one hot encoding for training\n",
      "X_train shape:  torch.Size([413544, 300, 4])\n",
      "Y_train shape:  torch.Size([413544])\n",
      "Start one hot encoding for validation\n",
      "End one hot encoding for validation\n",
      "X_val shape:  torch.Size([51740, 300, 4])\n",
      "Y_val shape:  torch.Size([51740])\n",
      "Start one hot encoding for test\n",
      "End one hot encoding for test\n",
      "X_test shape:  torch.Size([51706, 300, 4])\n",
      "Y_test shape:  torch.Size([51706])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! MAIN\n",
    "\"\"\"\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), 'data'))\n",
    "rel_path_train = os.path.join(data_dir, 'fullset_train.csv')\n",
    "rel_path_val = os.path.join(data_dir, 'fullset_validation.csv')\n",
    "rel_path_test = os.path.join(data_dir, 'fullset_test.csv')\n",
    "\n",
    "# Training set\n",
    "\n",
    "# Read the input from the csv file\n",
    "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
    "# Drop the NaN values\n",
    "train_csv = train_csv.dropna()\n",
    "# Describe the data\n",
    "# print(train_csv.describe())\n",
    "\n",
    "# Get the data from the csv file\n",
    "train_data = train_csv.values\n",
    "# m = number of input samples\n",
    "m = train_data.shape[0]\n",
    "\n",
    "# Dataframe\n",
    "data = {'sequence' : train_data[:m,1],\n",
    "        'label' : train_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_train = df['sequence'].values\n",
    "y_train = df['label'].values\n",
    "\n",
    "#Convert the sequences to numeric\n",
    "numeric_train = [sequence_to_numeric(seq) for seq in X_train]\n",
    "numeric_train = np.array(numeric_train)\n",
    "\n",
    "# Reshape the data\n",
    "n_samples, _ = numeric_train.shape\n",
    "numeric_train_sequences = numeric_train.reshape((n_samples, -1))\n",
    "\n",
    "# Apply the SMOTE algorithm to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_resample(numeric_train_sequences, y_train)\n",
    "\n",
    "#Convert the sequences to string\n",
    "X_train = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_train]\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for training\")\n",
    "X_train = onehot_encoder(X_train)\n",
    "print(\"End one hot encoding for training\")\n",
    "\n",
    "X_train = th.from_numpy(X_train).to(device)\n",
    "Y_train = th.tensor(y_train).to(device)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "\n",
    "# Validation set\n",
    "# Read the input from the csv file\n",
    "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
    "# Drop the NaN values\n",
    "val_csv = val_csv.dropna()\n",
    "\n",
    "val_data = val_csv.values\n",
    "# m = number of input samples\n",
    "m = val_data.shape[0]\n",
    "\n",
    "# Dataframe and upsample\n",
    "data = {'sequence' : val_data[:m,1],\n",
    "        'label' : val_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_val = df['sequence'].values\n",
    "y_val = df['label'].values\n",
    "\n",
    "#Convert the sequences to numeric\n",
    "numeric_val = [sequence_to_numeric(seq) for seq in X_val]\n",
    "numeric_val = np.array(numeric_val)\n",
    "\n",
    "# Reshape the data\n",
    "n_samples, _ = numeric_val.shape\n",
    "numeric_val_sequences = numeric_val.reshape((n_samples, -1))\n",
    "\n",
    "# Apply the SMOTE algorithm to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_val, y_val = smote.fit_resample(numeric_val_sequences, y_val)\n",
    "\n",
    "#Convert the sequences to string\n",
    "X_val = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_val]\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for validation\")\n",
    "X_val = onehot_encoder(X_val)\n",
    "print(\"End one hot encoding for validation\")\n",
    "\n",
    "X_val = th.from_numpy(X_val).to(device)\n",
    "Y_val = th.tensor(y_val).to(device)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"Y_val shape: \", Y_val.shape)\n",
    "\n",
    "# Test set\n",
    "# Read the input from the csv file\n",
    "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
    "# Drop the NaN values\n",
    "test_csv = test_csv.dropna()\n",
    "\n",
    "test_data = test_csv.values\n",
    "# m = number of input samples\n",
    "m = test_data.shape[0]\n",
    "\n",
    "# Dataframe and upsample\n",
    "data = {'sequence' : test_data[:m,1],\n",
    "        'label' : test_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_test = df['sequence'].values\n",
    "y_test = df['label'].values\n",
    "\n",
    "#Convert the sequences to numeric\n",
    "numeric_test = [sequence_to_numeric(seq) for seq in X_test]\n",
    "numeric_test = np.array(numeric_test)\n",
    "\n",
    "# Reshape the data\n",
    "n_samples, _ = numeric_test.shape\n",
    "numeric_test_sequences = numeric_test.reshape((n_samples, -1))\n",
    "\n",
    "# Apply the SMOTE algorithm to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_test, y_test = smote.fit_resample(numeric_test_sequences, y_test)\n",
    "\n",
    "#Convert the sequences to string\n",
    "X_test = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_test]\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for test\")\n",
    "X_test = onehot_encoder(X_test)\n",
    "print(\"End one hot encoding for test\")\n",
    "\n",
    "X_test = th.from_numpy(X_test).to(device)\n",
    "Y_test = th.tensor(y_test).to(device)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)\n",
    "\n",
    "#free memory\n",
    "del train_csv, val_csv, test_csv, train_data, val_data, test_data, data, df, numeric_train, numeric_val, numeric_test, numeric_train_sequences, numeric_val_sequences, numeric_test_sequences, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Data Loader with X label and Y label together\n",
    "\"\"\"\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return data_point, label\n",
    "\n",
    "# Create the Dataset\n",
    "train_dataset = CreateDataset(X_train, Y_train)\n",
    "val_dataset = CreateDataset(X_val, Y_val)\n",
    "test_dataset = CreateDataset(X_test, Y_test)\n",
    "\n",
    "# Batch size\n",
    "batch_dim = 128\n",
    "\n",
    "# Create the Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_dim, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=True)\n",
    "\n",
    "# Free memory\n",
    "del X_train, X_val, Y_val, X_test\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the LSTM\n",
      "Epoch [1/100], Train Loss: 0.1412s\n",
      "Epoch [2/100], Train Loss: 0.1107s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pietr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training the LSTM\")\n",
    "\n",
    "# Model, loss function and optimizer\n",
    "model_LSTM = LSTM_Net().to(device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # CHECK THE VALUE!!\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_LSTM.train()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "\n",
    "        X_batch = X_batch.float().to(device)\n",
    "        Y_batch = Y_batch.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_LSTM(X_batch.to(device))\n",
    "        loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "        \"\"\"\n",
    "        scaler = th.cuda.amp.GradScaler()\n",
    "        with th.cuda.amp.autocast():\n",
    "          outputs = model_LSTM(X_batch.to(device))\n",
    "          loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\"\"\"\n",
    "\n",
    "        # Free memory\n",
    "        del X_batch, Y_batch\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    # Validation\n",
    "    model_LSTM.eval()\n",
    "    val_loss = 0.0\n",
    "    j = 0\n",
    "    with th.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch = X_batch.float()\n",
    "            Y_batch = Y_batch.long()\n",
    "\n",
    "            outputs = model_LSTM(X_batch.to(device))\n",
    "            loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            j += 1\n",
    "\n",
    "            # Free memory\n",
    "            del X_batch, Y_batch\n",
    "            th.cuda.empty_cache()\n",
    "\n",
    "    # Losses\n",
    "    running_loss = running_loss/i\n",
    "    val_loss = val_loss/j\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"End training the LSTM\")\n",
    "\n",
    "# Free memory\n",
    "del i, j, running_loss, val_loss\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
