# -*- coding: utf-8 -*-
"""project_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/FrancescoChemello/Deep-Learning-Project/blob/main/code/project_colab.ipynb

Deep learning project aa 2023/24

Libraries
"""

import os
import threading
import time
import sys

import torch as th
import torch.nn as nn
import numpy as np
import pandas as pd
import tensorflow as tf

"""Function that perform the onehot encoding"""

def onehot_encoder(dataset):
    """
    Function that encodes a DNA dataset into a onehot encoding dataset.
    """
    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]
    onehot_dataset_numpy = np.array(onehot_dataset)

    return onehot_dataset_numpy


def dna_onehot_encoder(dna_sequence):
    """
    Function that encodes a single DNA string into a onehot encoding string.
    """
    onehot_dict = {
        'A' : [1, 0, 0, 0],
        'C' : [0, 1, 0, 0],
        'G' : [0, 0, 1, 0],
        'T' : [0, 0, 0, 1]
    }
    encoder = [onehot_dict[nuc] for nuc in dna_sequence]

    return encoder

"""Data encoding"""

#!- MAIN

# Set the device to be used (GPU or CPU)
device = th.device("cuda" if th.cuda.is_available() else "cpu")
print("Device: ", device)

# Read the input from the cvc file
rel_path_train = 'data/fullset_train.csv'
rel_path_val = 'data/fullset_validation.csv'
rel_path_test = 'data/fullset_test.csv'

absolute_path = os.path.dirname(__file__)

# Training Set

# Read the input from the csv file
train_csv = pd.read_csv(os.path.join(absolute_path, rel_path_train), sep=",")
# Drop the NaN values
train_csv = train_csv.dropna()
# Describe the data
train_csv.describe()

# Get the data from the csv file
train_data = train_csv.values
# m = number of input samples
m = train_data.shape[0]
print("Amount of data:",m)
X_train = train_data[:m,1]
Y_train = train_data[:m,2].astype(np.int32)

# OneHot encoding for the training data
print("Start onehot encoding for the training data")
X_train = onehot_encoder(X_train)

# Convert the data to a tensor
X_train = th.from_numpy(X_train)
Y_train = th.tensor(Y_train)

print("X_train shape: ", X_train.shape)
print("Y_train shape: ", Y_train.shape)

# Free memory
del train_csv, train_data, m

# Validation Set

# Read the input from the csv file
val_csv = pd.read_csv(os.path.join(absolute_path, rel_path_val), sep=",")
# Drop the NaN values
val_csv = val_csv.dropna()
# Describe the data
val_csv.describe()

val_data = val_csv.values
# m = number of input samples
m = val_data.shape[0]
print("Amount of data:",m)
X_val = val_data[:m,1]
Y_val = val_data[:m,2].astype(np.int32)

# OneHot encoding for the validation data
print("Start onehot encoding for the validation data")
X_val = onehot_encoder(X_val)

X_val = th.from_numpy(X_val)
Y_val = th.tensor(Y_val)

print("X_val shape", X_val.shape)
print("Y_val shape", Y_val.shape)

# Free memory
del val_csv, val_data, m

# Test

# Read the input from the csv file
test_csv = pd.read_csv(os.path.join(absolute_path, rel_path_test), sep=",")
# Drop the NaN values
test_csv = test_csv.dropna()
# Describe the data
test_csv.describe()

test_data = test_csv.values
# m = number of input samples
m = test_data.shape[0]
print("Amount of data:",m)
X_test = test_data[:m,1]
Y_test = test_data[:m,2].astype(np.int32)

# OneHot encoding for the test data
print("Start onehot encoding for the test data")
X_test = onehot_encoder(X_test)

X_test = th.from_numpy(X_test)
Y_test = th.tensor(Y_test)

print("X_test shape", X_test.shape)
print("Y_test shape", Y_test.shape)

# Free memory
del test_csv, test_data, m

"""Train the model"""

print("Start training the model")

exit() # Remove this line

# RNN

# 1. Define the model
# Hyperparameters for the model (need a vector in a k-fold validation):
#   input_size – The number of expected features in the input x
#   hidden_size – The number of features in the hidden state h
#   num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1
#   bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True
#   batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False
#   dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0
#   bidirectional – If True, becomes a bidirectional GRU. Default:

input_size = 4  # A, C, G, T
hidden_size = 64  # To be defined
output_size = 2  # We want a probabilistic value
num_layers = 1  # Just one GRU and not n GRU stacked
bias = True # We want to use bias
batch_first = False # The input and output tensors are provided as (seq, batch, feature)
dropout = 0.0 # No dropout
bidirectional = True # We want a bidirectional GRU

# INPUT: input, h_0
# -input: tensor shape (L, H_in) where L is the sequence length and H_in is the input size.
# -h_0: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.

# OUTPUT: output, h_n
# -output: tensor shape (L, D*H_out) where L is the sequence length, D is 2 for bidirectional and H_out is the hidden size.
# -h_n: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.

learning_rate = 0.001
num_classes = 2

model = th.nn.GRU(input_size, hidden_size, num_layers, num_classes, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, device=None, dtype=None).to(device)
criterion = nn.CrossEntropyLoss()   #the most common loss function used for classification problems
optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)

# 2. Train the model with epochs
# for epoch in range(num_epochs):
#     # TODO: Implement the training loop
#     break

model.train()
# 3. Test the model using epochs

print("End of training the model")

print("Start testing the model")

# 4. Evaluate the model
predicted_values, _ = model(test_data)

print("End of testing the model")