{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lP0Ty_LPOPx"
      },
      "source": [
        "Deep learning project aa 2023/24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXCvJMNiPWPb"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czu9e1vBNszI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# For the progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qIn2_aoP_Om"
      },
      "source": [
        "Function that perform the onehot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBQaLGJ9NQNK"
      },
      "outputs": [],
      "source": [
        "def onehot_encoder(dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    \"\"\"\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    \"\"\"\n",
        "    Function that encodes a single DNA string into a onehot encoding string.\n",
        "    \"\"\"\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "\n",
        "    return encoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQstKCYgQSfe"
      },
      "source": [
        "Data encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOgkug5EPX11",
        "outputId": "578c6231-8ee1-44a7-904c-af78ecfdefb6"
      },
      "outputs": [],
      "source": [
        "#! - TESTER\n",
        "path = './data/fullset_test.csv'\n",
        "\n",
        "# Read the input from the csv file\n",
        "csv = pd.read_csv(path, sep=\",\")\n",
        "# Drop the NaN values\n",
        "csv = csv.dropna()\n",
        "# Describe the data\n",
        "csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "data = csv.values\n",
        "# m = number of input samples\n",
        "m = data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X = data[:m,1]\n",
        "Y = data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "m = 5\n",
        "X = X[:m]\n",
        "Y = Y[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X = onehot_encoder(X)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X = th.tensor(X)\n",
        "Y = th.tensor(Y)\n",
        "\n",
        "print(\"X_train shape: \", X.shape)\n",
        "print(\"Y_train shape: \", Y.shape)\n",
        "\n",
        "# Free memory\n",
        "del csv, data, m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gYkIfnYQRGU",
        "outputId": "37ab12f3-c013-4a3a-bd5e-1239c6a8b35f"
      },
      "outputs": [],
      "source": [
        "#!- MAIN\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "rel_path_train = './data/fullset_train.csv'\n",
        "rel_path_val = './data/fullset_validation.csv'\n",
        "rel_path_test = './data/fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "train_csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_train = train_data[:m,1]\n",
        "Y_train = train_data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "# m = 100000\n",
        "# X_train = X_train[:m]\n",
        "# Y_train = Y_train[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train)\n",
        "Y_train = th.tensor(Y_train)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "val_csv.describe()\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val)\n",
        "Y_val = th.tensor(Y_val)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "test_csv.describe()\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test)\n",
        "Y_test = th.tensor(Y_test)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRU, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(300, 16, kernel_size=3, padding=1) \n",
        "        self.pool1 = nn.MaxPool1d(2)  \n",
        "        self.gru = nn.GRU(16, 64, num_layers=2, bidirectional=True) \n",
        "        self.fc1 = nn.Linear(256, 128) \n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = self.pool1(x)\n",
        "        print(x.shape)\n",
        "        x = x.permute(0, 2, 1) \n",
        "        print(x.shape)\n",
        "        x, _ = self.gru(x)\n",
        "        print(x.shape)\n",
        "        x = x.flatten(1)\n",
        "        print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        print(x.shape)\n",
        "        x = self.fc2(x)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "    \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(300, 16, kernel_size=3, padding=1) \n",
        "        self.pool1 = nn.MaxPool1d(2)  \n",
        "        self.lstm = nn.LSTM(16, 64, num_layers=2, bidirectional=True) \n",
        "        self.fc1 = nn.Linear(256, 128) \n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        print(x.shape)\n",
        "        x = self.pool1(x)\n",
        "        print(x.shape)\n",
        "        x = x.permute(0, 2, 1) \n",
        "        print(x.shape)\n",
        "        x, _ = self.lstm(x)\n",
        "        print(x.shape)\n",
        "        x = x.flatten(1)\n",
        "        print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        print(x.shape)\n",
        "        x = self.fc2(x)\n",
        "        print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSnStN2NQbRh"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENKk0h2VQZiR"
      },
      "outputs": [],
      "source": [
        "GRU_model = GRU().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.Adam(GRU_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 1\n",
        "\n",
        "#Store the loss values\n",
        "train_losses_GRU, val_losses_GRU = [], []\n",
        "for epoch in range(num_epochs):\n",
        "    GRU_model.train()\n",
        "    X_train = X_train.float()\n",
        "    Y_train = Y_train.long()\n",
        "    outputs = GRU_model(X_train.to(device))\n",
        "    loss = criterion(outputs, Y_train.to(device))\n",
        "    train_losses_GRU.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Validation\n",
        "    GRU_model.eval()\n",
        "    with th.no_grad():\n",
        "        X_val = X_val.float()\n",
        "        Y_val = Y_val.long()\n",
        "        val_outputs = GRU_model(X_val.to(device))\n",
        "        val_loss = criterion(val_outputs, Y_val.to(device))\n",
        "        val_losses_GRU.append(loss.item())\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LSTM_model = LSTM().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.Adam(LSTM_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 1\n",
        "\n",
        "#Store the loss values\n",
        "train_losses_LSTM, val_losses_LSTM = [], []\n",
        "for epoch in range(num_epochs):\n",
        "    LSTM_model.train()\n",
        "    X_train = X_train.float()\n",
        "    Y_train = Y_train.long()\n",
        "    outputs = LSTM_model(X_train.to(device))\n",
        "    loss = criterion(outputs, Y_train.to(device))\n",
        "    train_losses_LSTM.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Validation\n",
        "    LSTM_model.eval()\n",
        "    with th.no_grad():\n",
        "        X_val = X_val.float()\n",
        "        Y_val = Y_val.long()\n",
        "        val_outputs = LSTM_model(X_val.to(device))\n",
        "        val_loss = criterion(val_outputs, Y_val.to(device))\n",
        "        val_losses_LSTM.append(loss.item())\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
