{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning project aa 2023/24"
      ],
      "metadata": {
        "id": "-lP0Ty_LPOPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "RXCvJMNiPWPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "czu9e1vBNszI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# For the progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that perform the onehot encoding"
      ],
      "metadata": {
        "id": "8qIn2_aoP_Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_encoder(dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    \"\"\"\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    \"\"\"\n",
        "    Function that encodes a single DNA string into a onehot encoding string.\n",
        "    \"\"\"\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "\n",
        "    return encoder\n"
      ],
      "metadata": {
        "id": "dBQaLGJ9NQNK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding"
      ],
      "metadata": {
        "id": "CQstKCYgQSfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! - TESTER\n",
        "path = '/content/data/fullset_test.csv'\n",
        "\n",
        "# Read the input from the csv file\n",
        "csv = pd.read_csv(path, sep=\",\")\n",
        "# Drop the NaN values\n",
        "csv = csv.dropna()\n",
        "# Describe the data\n",
        "csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "data = csv.values\n",
        "# m = number of input samples\n",
        "m = data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X = data[:m,1]\n",
        "Y = data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "m = 5\n",
        "X = X[:m]\n",
        "Y = Y[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X = onehot_encoder(X)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X = th.tensor(X)\n",
        "Y = th.tensor(Y)\n",
        "\n",
        "print(\"X_train shape: \", X.shape)\n",
        "print(\"Y_train shape: \", Y.shape)\n",
        "\n",
        "# Free memory\n",
        "del csv, data, m"
      ],
      "metadata": {
        "id": "AOgkug5EPX11",
        "outputId": "578c6231-8ee1-44a7-904c-af78ecfdefb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amount of data: 26404\n",
            "Start onehot encoding for the training data\n",
            "X before the conversion to ternsor\n",
            "[[[0 0 1 0]\n",
            "  [0 0 1 0]\n",
            "  [0 1 0 0]\n",
            "  ...\n",
            "  [0 1 0 0]\n",
            "  [1 0 0 0]\n",
            "  [0 1 0 0]]\n",
            "\n",
            " [[0 0 0 1]\n",
            "  [0 0 1 0]\n",
            "  [1 0 0 0]\n",
            "  ...\n",
            "  [1 0 0 0]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 0]]\n",
            "\n",
            " [[1 0 0 0]\n",
            "  [1 0 0 0]\n",
            "  [0 1 0 0]\n",
            "  ...\n",
            "  [0 0 1 0]\n",
            "  [1 0 0 0]\n",
            "  [0 0 1 0]]\n",
            "\n",
            " [[0 0 1 0]\n",
            "  [1 0 0 0]\n",
            "  [0 0 1 0]\n",
            "  ...\n",
            "  [0 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [1 0 0 0]]\n",
            "\n",
            " [[0 1 0 0]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 0]\n",
            "  ...\n",
            "  [0 0 1 0]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 0]]]\n",
            "X_train shape:  torch.Size([5, 300, 4])\n",
            "Y_train shape:  torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!- MAIN\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "rel_path_train = '/content/data/fullset_train.csv'\n",
        "rel_path_val = '/content/data/fullset_validation.csv'\n",
        "rel_path_test = '/content/data/fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "train_csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_train = train_data[:m,1]\n",
        "Y_train = train_data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "# m = 100000\n",
        "# X_train = X_train[:m]\n",
        "# Y_train = Y_train[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train)\n",
        "Y_train = th.tensor(Y_train)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "val_csv.describe()\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val)\n",
        "Y_val = th.tensor(Y_val)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "test_csv.describe()\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test)\n",
        "Y_test = th.tensor(Y_test)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gYkIfnYQRGU",
        "outputId": "37ab12f3-c013-4a3a-bd5e-1239c6a8b35f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n",
            "Amount of data: 211238\n",
            "Start onehot encoding for the training data\n",
            "X_train shape:  torch.Size([211238, 300, 4])\n",
            "Y_train shape:  torch.Size([211238])\n",
            "Amount of data: 26404\n",
            "Start onehot encoding for the validation data\n",
            "X_val shape torch.Size([26404, 300, 4])\n",
            "Y_val shape torch.Size([26404])\n",
            "Amount of data: 26404\n",
            "Start onehot encoding for the test data\n",
            "X_test shape torch.Size([26404, 300, 4])\n",
            "Y_test shape torch.Size([26404])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "HSnStN2NQbRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the model\")\n",
        "\n",
        "# RNN\n",
        "\n",
        "# 1. Define the model\n",
        "# Hyperparameters for the model (need a vector in a k-fold validation):\n",
        "#   input_size – The number of expected features in the input x\n",
        "#   hidden_size – The number of features in the hidden state h\n",
        "#   num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
        "#   bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "#   batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
        "#   dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "#   bidirectional – If True, becomes a bidirectional GRU. Default:\n",
        "\n",
        "input_size = 4  # A, C, G, T\n",
        "hidden_size = 64  # To be defined\n",
        "output_size = 2  # We want a probabilistic value\n",
        "num_layers = 1  # Just one GRU and not n GRU stacked\n",
        "bias = True # We want to use bias\n",
        "batch_first = False # The input and output tensors are provided as (seq, batch, feature)\n",
        "dropout = 0.0 # No dropout\n",
        "bidirectional = True # We want a bidirectional GRU\n",
        "\n",
        "# INPUT: input, h_0\n",
        "# -input: tensor shape (L, H_in) where L is the sequence length and H_in is the input size.\n",
        "# -h_0: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "# OUTPUT: output, h_n\n",
        "# -output: tensor shape (L, D*H_out) where L is the sequence length, D is 2 for bidirectional and H_out is the hidden size.\n",
        "# -h_n: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "\n",
        "model = th.nn.GRU(input_size, hidden_size, num_layers, num_classes, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, device=None, dtype=None).to(device)\n",
        "criterion = nn.CrossEntropyLoss()   #the most common loss function used for classification problems\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 2. Train the model with epochs\n",
        "# for epoch in range(num_epochs):\n",
        "#     # TODO: Implement the training loop\n",
        "#     break\n",
        "\n",
        "model.train()\n",
        "# 3. Test the model using epochs\n",
        "\n",
        "print(\"End of training the model\")\n",
        "\n",
        "print(\"Start testing the model\")\n",
        "\n",
        "# 4. Evaluate the model\n",
        "predicted_values, _ = model(test_data)\n",
        "\n",
        "print(\"End of testing the model\")"
      ],
      "metadata": {
        "id": "ENKk0h2VQZiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}