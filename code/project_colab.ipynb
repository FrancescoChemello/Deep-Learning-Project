{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning project aa 2023/24"
      ],
      "metadata": {
        "id": "-lP0Ty_LPOPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "RXCvJMNiPWPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "czu9e1vBNszI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "# For the progress bar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that perform the onehot encoding"
      ],
      "metadata": {
        "id": "8qIn2_aoP_Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_onehot_encoder(dataset):\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "    pbar.close()\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "    pbar.update(1)\n",
        "    return encoder\n",
        "\n"
      ],
      "metadata": {
        "id": "dBQaLGJ9NQNK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def onehot_encoder (dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    The dataset is a list of strings, each string is a DNA sequence.\n",
        "    The function returns a list of lists, each list is a onehot encoding of a DNA sequence.\n",
        "    \"\"\"\n",
        "    # Define the dictionary for the onehot encoding\n",
        "    onehot_dict = {'A':[1,0,0,0], 'C':[0,1,0,0], 'G':[0,0,1,0], 'T':[0,0,0,1]}\n",
        "    # Define the chunk size for the export of the data to a numpy array\n",
        "    chunk_size = 10000\n",
        "    # Initialize the onehot dataset\n",
        "    onehot_dataset = []\n",
        "    # Iterate over the dataset\n",
        "    pbar = tqdm(total=len(dataset))\n",
        "    for sequence in dataset:\n",
        "        # Initialize the onehot sequence\n",
        "        onehot_sequence = np.array([])\n",
        "        # Iterate over the sequence\n",
        "        for base in sequence:\n",
        "            # Append the onehot base\n",
        "            onehot_sequence = np.append(onehot_sequence, onehot_dict[base])\n",
        "        # Append the onehot sequence\n",
        "        onehot_dataset.append(onehot_sequence)\n",
        "        pbar.update(1)\n",
        "    # Convert the onehot dataset to a numpy array to have an arry of arrays\n",
        "    pbar.close()\n",
        "    print(\"Starting convertion to numpy array\")\n",
        "    # Convert it into a numpy array\n",
        "    onehot_dataset_numpy = np.asarray(onehot_dataset)\n",
        "    print(\"Type of onehot_dataset_numpy: \", type(onehot_dataset_numpy))\n",
        "    print(\"Shape of onehot_dataset_numpy: \", onehot_dataset_numpy.shape)\n",
        "    print(\"Onehot encoding done\")\n",
        "    return onehot_dataset_numpy"
      ],
      "metadata": {
        "id": "Jbw00zSUP9u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding"
      ],
      "metadata": {
        "id": "CQstKCYgQSfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! - TESTER\n",
        "path = '/content/data/fullset_test.csv'\n",
        "\n",
        "# Read the input from the csv file\n",
        "csv = pd.read_csv(path, sep=\",\")\n",
        "# Drop the NaN values\n",
        "csv = csv.dropna()\n",
        "# Describe the data\n",
        "csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "data = csv.values\n",
        "# m = number of input samples\n",
        "m = data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X = data[:m,1]\n",
        "Y = data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "m = 5\n",
        "X = X[:m]\n",
        "Y = Y[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = new_onehot_encoder(X)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X)\n",
        "Y_train = th.tensor(Y)\n",
        "\n",
        "print(\"X_train shape: \", X.shape)\n",
        "print(\"Y_train shape: \", Y.shape)\n",
        "\n",
        "# Free memory\n",
        "del csv, data, m"
      ],
      "metadata": {
        "id": "AOgkug5EPX11",
        "outputId": "844ed707-2b7a-4ad6-9711-edaa3545f7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/fullset_test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9b390b1ec509>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read the input from the csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Drop the NaN values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/fullset_test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!- MAIN\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "rel_path_train = '/content/data/fullset_train.csv'\n",
        "rel_path_val = '/content/data/fullset_validation.csv'\n",
        "rel_path_test = '/content/data/fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "train_csv.describe()\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_train = train_data[:m,1]\n",
        "Y_train = train_data[:m,2].astype(np.int32)\n",
        "\n",
        "# # Reduce the size of the dataset for testing\n",
        "# m = 100000\n",
        "# X_train = X_train[:m]\n",
        "# Y_train = Y_train[:m]\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train)\n",
        "Y_train = th.tensor(Y_train)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "\n",
        "exit() # Debug -END OF ONEHOT ENCODING-\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "val_csv.describe()\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val)\n",
        "Y_val = th.tensor(Y_val)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "test_csv.describe()\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "print(\"Amount of data:\",m)\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test)\n",
        "Y_test = th.tensor(Y_test)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gYkIfnYQRGU",
        "outputId": "443156a5-33f6-447d-f9c6-35f7b4be3aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cpu\n",
            "Amount of data: 211238\n",
            "Start onehot encoding for the training data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 211238/211238 [04:21<00:00, 809.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting convertion to numpy array\n",
            "Type of onehot_dataset_numpy:  <class 'numpy.ndarray'>\n",
            "Shape of onehot_dataset_numpy:  (211238, 1200)\n",
            "Onehot encoding done\n",
            "X_train shape:  torch.Size([211238, 1200])\n",
            "Y_train shape:  torch.Size([211238])\n",
            "Amount of data: 26404\n",
            "Start onehot encoding for the validation data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26404/26404 [00:32<00:00, 814.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting convertion to numpy array\n",
            "Type of onehot_dataset_numpy:  <class 'numpy.ndarray'>\n",
            "Shape of onehot_dataset_numpy:  (26404, 1200)\n",
            "Onehot encoding done\n",
            "X_val shape torch.Size([26404, 1200])\n",
            "Y_val shape torch.Size([26404])\n",
            "Amount of data: 26404\n",
            "Start onehot encoding for the test data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26404/26404 [00:31<00:00, 830.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting convertion to numpy array\n",
            "Type of onehot_dataset_numpy:  <class 'numpy.ndarray'>\n",
            "Shape of onehot_dataset_numpy:  (26404, 1200)\n",
            "Onehot encoding done\n",
            "X_test shape torch.Size([26404, 1200])\n",
            "Y_test shape torch.Size([26404])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "HSnStN2NQbRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the model\")\n",
        "\n",
        "# RNN\n",
        "\n",
        "# 1. Define the model\n",
        "# Hyperparameters for the model (need a vector in a k-fold validation):\n",
        "#   input_size – The number of expected features in the input x\n",
        "#   hidden_size – The number of features in the hidden state h\n",
        "#   num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1\n",
        "#   bias – If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
        "#   batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False\n",
        "#   dropout – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
        "#   bidirectional – If True, becomes a bidirectional GRU. Default:\n",
        "\n",
        "input_size = 4  # A, C, G, T\n",
        "hidden_size = 64  # To be defined\n",
        "output_size = 2  # We want a probabilistic value\n",
        "num_layers = 1  # Just one GRU and not n GRU stacked\n",
        "bias = True # We want to use bias\n",
        "batch_first = False # The input and output tensors are provided as (seq, batch, feature)\n",
        "dropout = 0.0 # No dropout\n",
        "bidirectional = True # We want a bidirectional GRU\n",
        "\n",
        "# INPUT: input, h_0\n",
        "# -input: tensor shape (L, H_in) where L is the sequence length and H_in is the input size.\n",
        "# -h_0: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "# OUTPUT: output, h_n\n",
        "# -output: tensor shape (L, D*H_out) where L is the sequence length, D is 2 for bidirectional and H_out is the hidden size.\n",
        "# -h_n: tensor shape (D*num_layers, H_out) where D is 2 for bidirectional and H_out is the hidden size.\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_classes = 2\n",
        "\n",
        "model = th.nn.GRU(input_size, hidden_size, num_layers, num_classes, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, device=None, dtype=None).to(device)\n",
        "criterion = nn.CrossEntropyLoss()   #the most common loss function used for classification problems\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 2. Train the model with epochs\n",
        "# for epoch in range(num_epochs):\n",
        "#     # TODO: Implement the training loop\n",
        "#     break\n",
        "\n",
        "model.train()\n",
        "# 3. Test the model using epochs\n",
        "\n",
        "print(\"End of training the model\")\n",
        "\n",
        "print(\"Start testing the model\")\n",
        "\n",
        "# 4. Evaluate the model\n",
        "predicted_values, _ = model(test_data)\n",
        "\n",
        "print(\"End of testing the model\")"
      ],
      "metadata": {
        "id": "ENKk0h2VQZiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}