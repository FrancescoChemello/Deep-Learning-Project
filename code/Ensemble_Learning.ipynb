{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning**\n",
        "- **CNN**.\n",
        "- **RNN** implemented using a *LSTM*.\n",
        "\n",
        "It can uses also a *random seed* to shuffle the data and use a different *training*, *validation* and *test* set respect to the ones used by ***ViraMiner***."
      ],
      "metadata": {
        "id": "XesKIRwki-SK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kfMZfDCGW1B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main libraries\n",
        "\"\"\"\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For the ensamble learning\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "aTJgc4Axfk7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Only for the metrics analysis\n",
        "\"\"\"\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "MRQnBXcPjfBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Onehot Encoding** that transform a *DNA sequence* to a vector of zeros and ones. It is useful expectially for the **CNN**."
      ],
      "metadata": {
        "id": "V8Z62Tnwjifl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions for the onehot encoding\n",
        "\"\"\"\n",
        "\n",
        "def onehot_encoder(dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    \"\"\"\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    \"\"\"\n",
        "    Function that encodes a single DNA string into a onehot encoding string.\n",
        "    \"\"\"\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "\n",
        "    return encoder"
      ],
      "metadata": {
        "id": "UvkvwCS0jftS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yrYP1-P5huvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViraMinerDataset (Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def one_hot_encode(self, sequence):\n",
        "        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "        seq_length = len(sequence)\n",
        "        one_hot = th.zeros((seq_length, 4), dtype=th.float32)\n",
        "        for i, base in enumerate(sequence):\n",
        "            one_hot[i, mapping[base]] = 1\n",
        "\n",
        "        return one_hot\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.sequences[idx]\n",
        "        label = self.labels[idx]\n",
        "        one_hot_sequence = self.one_hot_encode(sequence)\n",
        "        return one_hot_sequence, label"
      ],
      "metadata": {
        "id": "__eeqG__hvSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **CNN** model: direct from the file *CNN.ipynb* avaiable in the GitHub repository."
      ],
      "metadata": {
        "id": "9xFuLfEFkTJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!- CNN Model\n",
        "\"\"\"\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    # We can use a differnet pool for each layer\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(300, 200, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv1d(200, 100, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv1d(100, 75, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(75),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv1d(75, 50, kernel_size = 2, padding = 1),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv1d(50, 32, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        self.linear1 = nn.Linear(32, 128)\n",
        "        self.linear2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        # print(\"Layer 1: \", x.shape)\n",
        "        x = self.layer2(x)\n",
        "        # print(\"Layer 2: \", x.shape)\n",
        "        x = self.layer3(x)\n",
        "        # print(\"Layer 3: \", x.shape)\n",
        "        x = self.layer4(x)\n",
        "        # print(\"Layer 4: \", x.shape)\n",
        "        x = self.layer5(x)\n",
        "        # print(\"Layer 5: \", x.shape)\n",
        "        # Flatten the output for the linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        # print(\"Linear: \", x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "u-4GD7XkkRiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **RNN** model: direct from the file *LSTM.ipynb* avaiable in the GitHub repository. We choose the *LSTM* over the *GRU* because it obtained an **higher** precision on the dataset and also is **less** computationally demanding."
      ],
      "metadata": {
        "id": "Xd0t15Gjke_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Net (nn.Module):\n",
        "    def __init__ (self):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=2, batch_first=True, dropout=0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CFVonbFAkd7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN**"
      ],
      "metadata": {
        "id": "gOdTxRjflNbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training phase"
      ],
      "metadata": {
        "id": "uOcE4IfZlXCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!- MAIN\n",
        "\"\"\"\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "\"\"\"\n",
        "If run in local machine:\n",
        "\n",
        "data_dir = os.path.abspath(os.path.join(os.getcwd(), 'data'))\n",
        "train_data_path = os.path.join(data_dir, 'fullset_train.csv')\n",
        "val_data_path = os.path.join(data_dir, 'fullset_validation.csv')\n",
        "test_data_path = os.path.join(data_dir, 'fullset_test.csv')\n",
        "\"\"\"\n",
        "\n",
        "rel_path_train = '/content/fullset_train.csv'\n",
        "rel_path_val = '/content/fullset_validation.csv'\n",
        "rel_path_test = '/content/fullset_test.csv'\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "print(train_csv.describe())\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "\n",
        "X_train = train_data[:m,1]\n",
        "Y_train = train_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train).to(device)\n",
        "Y_train = th.tensor(Y_train).to(device)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "del train_csv, train_data, m\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "print(val_csv.describe())\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val).to(device)\n",
        "Y_val = th.tensor(Y_val).to(device)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "print(test_csv.describe())\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test).to(device)\n",
        "Y_test = th.tensor(Y_test).to(device)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m"
      ],
      "metadata": {
        "id": "4TJX4a7QlZB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffle of the data** *(optional)* using a random seed."
      ],
      "metadata": {
        "id": "pyFnowCTl6p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "I merge the three tensor array to a big one and then, after a shuffle, I split the data into:\n",
        "  - Training: 211238 data\n",
        "  - Validation: 26404 data\n",
        "  - Test: 26404 data\n",
        "\n",
        "  Random seed: 47 (the most used)\n",
        "\"\"\"\n",
        "\n",
        "# Merge the arrays\n",
        "\n",
        "print(\"Shape of X_train: \", X_train.shape)\n",
        "print(\"Shape of X_val: \", X_val.shape)\n",
        "print(\"Shape of X_test: \", X_test.shape)\n",
        "\n",
        "X_data = th.concat((X_train, X_val), axis = 0).to(device)\n",
        "X_data = th.concat((X_data, X_test), axis = 0).to(device)\n",
        "\n",
        "print(\"Shape of data: \", X_data.shape)\n",
        "\n",
        "del X_train, X_val, X_test\n",
        "\n",
        "print(\"Shape of Y_train: \", Y_train.shape)\n",
        "print(\"Shape of Y_val: \", Y_val.shape)\n",
        "print(\"Shape of Y_test: \", Y_test.shape)\n",
        "\n",
        "Y_data = th.concat((Y_train, Y_val), axis = 0).to(device)\n",
        "Y_data = th.concat((Y_data, Y_test), axis = 0).to(device)\n",
        "\n",
        "print(\"Shape of data: \", Y_data.shape)\n",
        "\n",
        "del Y_train, Y_val, Y_test\n",
        "\n",
        "# Random Seed\n",
        "random_seed = 47\n",
        "th.manual_seed(random_seed)\n",
        "\n",
        "X_index_shuffle = th.randperm(X_data.size(0))\n",
        "X_data_shuffled = X_data[X_index_shuffle]\n",
        "\n",
        "Y_index_shuffle = th.randperm(Y_data.size(0))\n",
        "Y_data_shuffled = Y_data[Y_index_shuffle]\n",
        "\n",
        "print(\"X_data_shuffled shape: \", X_data_shuffled.shape)\n",
        "print(\"Y_data_shuffled shape: \", Y_data_shuffled.shape)\n",
        "\n",
        "# Split data into Training, Validation and Test\n",
        "\n",
        "X_train = X_data_shuffled[:211238].to(device)\n",
        "Y_train = Y_data_shuffled[:211238].to(device)\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "X_val = X_data_shuffled[211238:237642]\n",
        "Y_val = Y_data_shuffled[211238:237642]\n",
        "print(\"X_val shape: \", X_val.shape)\n",
        "print(\"Y_val shape: \", Y_val.shape)\n",
        "\n",
        "X_test = X_data_shuffled[237642:].to(device)\n",
        "Y_test = Y_data_shuffled[237642:].to(device)\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"Y_test shape: \", Y_test.shape)"
      ],
      "metadata": {
        "id": "j2XRhmhll3qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train* of the **CNN** using the *Validation* set to check the performance."
      ],
      "metadata": {
        "id": "YsILBtnjmSD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start training the model\")\n",
        "\n",
        "model_CNN = ConvNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.AdamW(model_CNN.parameters(), lr=0.001)\n",
        "scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # CHECK THE VALUE!!\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
        "train_loss_CNN, val_loss_CNN = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_CNN.train()\n",
        "    X_train = X_train.float()\n",
        "    Y_train = Y_train.long()\n",
        "    outputs = model_CNN(X_train.to(device))\n",
        "    loss = criterion(outputs, Y_train.to(device))\n",
        "    train_loss_CNN.append(loss.item())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validation the model\n",
        "    model_CNN.eval()\n",
        "    with th.no_grad():\n",
        "        X_val = X_val.float()\n",
        "        Y_val = Y_val.long()\n",
        "        val_outputs = model_CNN(X_val.to(device))\n",
        "        val_loss = criterion(val_outputs, Y_val.to(device))\n",
        "        val_loss_CNN.append(val_loss.item())\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss <= best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"End training the model\")"
      ],
      "metadata": {
        "id": "7SC620P4mZ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(OPTIONAL):** *Test* results for the **CNN**."
      ],
      "metadata": {
        "id": "aL4g50f6ocxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the CNN\")\n",
        "\n",
        "X_test = X_test.to(device).float()\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_CNN.eval()\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    outputs = model_CNN(X_test)\n",
        "    _, predicted = th.max(outputs, 1)\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true = Y_test.cpu().numpy()\n",
        "y_pred = predicted.cpu().numpy()\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "\n",
        "print(\"End testing the CNN\")"
      ],
      "metadata": {
        "id": "CBBUkJquokqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Train* of the **RNN** using the *Validation* set to check the performance."
      ],
      "metadata": {
        "id": "1G5i7_hGmvV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, loss function and optimizer\n",
        "model_LSTM = LSTM_Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = th.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 7\n",
        "train_loss_RNN, val_loss_RNN = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_LSTM.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    X_train = X_train.float()\n",
        "    Y_train = Y_train.long()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model_LSTM(X_train.to(device))\n",
        "    loss = criterion(outputs, Y_train.to(device))\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model_LSTM.eval()\n",
        "    with th.no_grad():\n",
        "      X_val = X_val.float()\n",
        "      Y_val = Y_val.long()\n",
        "      X_val = X_val.to(device)\n",
        "      Y_val = Y_val.to(device)\n",
        "\n",
        "      val_outputs = model_CNN(X_val.to(device))\n",
        "      val_loss = criterion(val_outputs, Y_val.to(device))\n",
        "      val_loss_CNN.append(val_loss.item())\n",
        "      scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "8ePulED-mxXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(OPTIONAL):** *Test* results forthe **RNN**."
      ],
      "metadata": {
        "id": "RCAvQnFzotd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the LSTM\")\n",
        "\n",
        "X_test = X_test.to(device).float()\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_LSTM.eval()\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    outputs = model_LSTM(X_test)\n",
        "    _, predicted = th.max(outputs, 1)\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true = Y_test.cpu().numpy()\n",
        "y_pred = predicted.cpu().numpy()\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "\n",
        "print(\"End testing the LSTM\")"
      ],
      "metadata": {
        "id": "Fc4lNi6IoqON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of the **Ensemble Learning** using an **SVM** (stacking ensemble learing)."
      ],
      "metadata": {
        "id": "oOhwQoMemxyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prepare the data for the final evaluation using DataLoader\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Function that, given a model and a dataset, it returns all the predictions (outputs)\n",
        "\"\"\"\n",
        "def get_predictions(model_pred, dataset):\n",
        "    model_pred.eval()\n",
        "    all_preds = []\n",
        "    with th.no_grad():\n",
        "        for inputs, _ in dataset:\n",
        "            outputs = model_pred(inputs)\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "    return np.concatenate(all_preds)\n",
        "\n",
        "# Prepare the prediction for the training model\n",
        "CNN_train_preds = get_predictions(model_CNN, X_train)\n",
        "LSTM_train_preds = get_predictions(model_LSTM, X_train)\n",
        "\n",
        "# Concatenate the two inputs\n",
        "X_train_meta = np.hstack((CNN_train_preds, LSTM_train_preds))"
      ],
      "metadata": {
        "id": "5PlI9nhBiojA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "An SVM takes in input the results from the CNN and RNN and then it preforms the final classification.\n",
        "\n",
        "I have to use skitlearn to do the ensable learing because the library is only avaiable on it!\n",
        "\"\"\"\n",
        "\n",
        "# We use a polynomial SVM for the final classification\n",
        "model_SVM = SVC(kernel = 'poly', probability = True)\n",
        "\n",
        "model_SVM.fit(X_train_meta, Y_train.ravel())"
      ],
      "metadata": {
        "id": "f0X4pYiTm7Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test** of the performance."
      ],
      "metadata": {
        "id": "PaBpALEtmo0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Start testing the final model\")\n",
        "\n",
        "# I convert the data (if needed)\n",
        "y_true = y_test.cpu().numpy() if isinstance(y_test, th.Tensor) else y_test\n",
        "\n",
        "# Testing the meta-model: SVM\n",
        "y_pred = model_SVM.predict(X_test_meta)\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "\n",
        "print(\"End testing the final model\")"
      ],
      "metadata": {
        "id": "c8YnAeVfmlAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}