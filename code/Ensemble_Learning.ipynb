{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XesKIRwki-SK"
      },
      "source": [
        "**Ensemble Learning**\n",
        "- **CNN**.\n",
        "- **RNN** implemented using a *LSTM* and *GRU*.\n",
        "\n",
        "Full repository avaiable on: https://github.com/FrancescoChemello/Deep-Learning-Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kfMZfDCGW1B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main libraries\n",
        "\"\"\"\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "import gc\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zYXg7hrcbwD"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "For SMOTE & Upsample\n",
        "\"\"\"\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-IIWBytvMzn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "For the class weight\n",
        "\"\"\"\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTJgc4Axfk7G"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "For the ensamble learning\n",
        "\"\"\"\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRQnBXcPjfBL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Only for the metrics analysis\n",
        "\"\"\"\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Z62Tnwjifl"
      },
      "source": [
        "The **Onehot Encoding** that transform a *DNA sequence* to a vector of zeros and ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvkvwCS0jftS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions for the onehot encoding\n",
        "\"\"\"\n",
        "\n",
        "def onehot_encoder(dataset):\n",
        "    \"\"\"\n",
        "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
        "    \"\"\"\n",
        "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
        "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
        "\n",
        "    return onehot_dataset_numpy\n",
        "\n",
        "\n",
        "def dna_onehot_encoder(dna_sequence):\n",
        "    \"\"\"\n",
        "    Function that encodes a single DNA string into a onehot encoding string.\n",
        "    \"\"\"\n",
        "    onehot_dict = {\n",
        "        'A' : [1, 0, 0, 0],\n",
        "        'C' : [0, 1, 0, 0],\n",
        "        'G' : [0, 0, 1, 0],\n",
        "        'T' : [0, 0, 0, 1]\n",
        "    }\n",
        "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
        "\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89t-a71dbWnX"
      },
      "outputs": [],
      "source": [
        "def sequence_to_numeric(sequence):\n",
        "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "    return [mapping[nuc] for nuc in sequence]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xFuLfEFkTJA"
      },
      "source": [
        "The **CNN** model: direct from the file *CNN.ipynb* avaiable in the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-4GD7XkkRiZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!- CNN Model\n",
        "\"\"\"\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    # We can use a differnet pool for each layer\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv1d(300, 200, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out = nn.Dropout()\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv1d(200, 100, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.PReLU(),\n",
        "            nn.AvgPool1d(2),\n",
        "            nn.Dropout1d(0.45)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out_1 = nn.Dropout()\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv1d(100, 75, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(75),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv1d(75, 50, kernel_size = 2, padding = 1),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv1d(50, 32, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.PReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout1d(0.3)\n",
        "        )\n",
        "\n",
        "        # I remove random connection to help the convergency\n",
        "        self.drop_out_2 = nn.Dropout()\n",
        "\n",
        "        self.linear1 = nn.Linear(32, 128)\n",
        "        self.linear2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x=self.drop_out_1(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x=self.drop_out_2(x)\n",
        "        # Flatten the output for the linear layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd0t15Gjke_F"
      },
      "source": [
        "The **RNN** model: direct from the file *LSTM.ipynb* avaiable in the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFVonbFAkd7f"
      },
      "outputs": [],
      "source": [
        "class LSTM_Net (nn.Module):\n",
        "    def __init__ (self):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, num_layers=6, batch_first=True, dropout=0.5, bidirectional=True)\n",
        "        # self.lstm = nn.LSTM(input_size=4, hidden_size=128, num_layers=2, dropout=0.5, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        x = x[:, -1, :]\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0lF9GYxqMty"
      },
      "source": [
        "The **RNN** model: direct from the file *GRU.ipynb* avaiable in the GitHub repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAxUARPCqIaU"
      },
      "outputs": [],
      "source": [
        "class GRU_Net (nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRU_Net, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(300, 16, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.gru = nn.GRU(16, 64, num_layers=2, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.gru(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRvIm8lrB4A_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Class to create a Data Loader with X label and Y label together\n",
        "\"\"\"\n",
        "class CreateDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_point = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return data_point, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOdTxRjflNbX"
      },
      "source": [
        "**MAIN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBUGVAGmDMLT"
      },
      "source": [
        "Set **Hyperparameters** for the code:\n",
        "\n",
        "\n",
        "*   Select the path for the dataset:\n",
        "    1.   For local machine.\n",
        "    2.   For Colab enviroment.\n",
        "    3.   For Goole Drive.\n",
        "\n",
        "*   Number of data upsampled.\n",
        "*   Number of data generated by the SMOTE.\n",
        "*   Type of RNN:\n",
        "    1.   For the GRU.\n",
        "    2.   For the LSTM\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzqumBZAEAdn"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hyperparameters\n",
        "\"\"\"\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = 3\n",
        "\n",
        "# Number of upsampled\n",
        "num_upsampled = 40000\n",
        "\n",
        "# Number of SMOTE generated data\n",
        "num_smote = 80000\n",
        "\n",
        "# Type of RNN\n",
        "type_RNN = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOcE4IfZlXCZ"
      },
      "source": [
        "Training phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGKQF_88kqIW"
      },
      "source": [
        "Extraction of the data from the *.cvc* files, **oversample** and **SMOTE** of data from class 1 *(if the user select them)*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TJX4a7QlZB6",
        "outputId": "590525d0-5447-43bf-f679-84933a34cdad"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "!- MAIN\n",
        "\"\"\"\n",
        "\n",
        "# Set the device to be used (GPU or CPU)\n",
        "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", device)\n",
        "\n",
        "# Read the input from the cvc file\n",
        "train_data_path = \"\"\n",
        "val_data_path = \"\"\n",
        "test_data_path = \"\"\n",
        "\n",
        "if dataset_path == 1:\n",
        "    \"\"\"\n",
        "    If runs in local machine:\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Local path selected\")\n",
        "\n",
        "    data_dir = os.path.abspath(os.path.join(os.getcwd(), 'data'))\n",
        "    train_data_path = os.path.join(data_dir, 'fullset_train.csv')\n",
        "    val_data_path = os.path.join(data_dir, 'fullset_validation.csv')\n",
        "    test_data_path = os.path.join(data_dir, 'fullset_test.csv')\n",
        "elif dataset_path == 2:\n",
        "\n",
        "      print(\"Colab path selected\")\n",
        "\n",
        "      rel_path_train = '/content/fullset_train.csv'\n",
        "      rel_path_val = '/content/fullset_validation.csv'\n",
        "      rel_path_test = '/content/fullset_test.csv'\n",
        "elif dataset_path == 3:\n",
        "\n",
        "      print(\"Google Drive path selected\")\n",
        "\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive')\n",
        "\n",
        "      rel_path_train = '/content/drive/MyDrive/Colab Notebooks/fullset_train.csv'\n",
        "      rel_path_val = '/content/drive/MyDrive/Colab Notebooks/fullset_validation.csv'\n",
        "      rel_path_test = '/content/drive/MyDrive/Colab Notebooks/fullset_test.csv'\n",
        "else:\n",
        "      print(\"Not a valid path selected!\")\n",
        "      sys.exit(1)\n",
        "\n",
        "# Training Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
        "# Drop the NaN values\n",
        "train_csv = train_csv.dropna()\n",
        "# Describe the data\n",
        "print(train_csv.describe())\n",
        "\n",
        "# Get the data from the csv file\n",
        "train_data = train_csv.values\n",
        "# m = number of input samples\n",
        "m = train_data.shape[0]\n",
        "\n",
        "# Dataframe and upsample\n",
        "data = {'sequence' : train_data[:m,1],\n",
        "        'label' : train_data[:m,2].astype(np.int32) }\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "count_class_0, count_class_1 = df['label'].value_counts()\n",
        "\n",
        "print(\"Samples class 0: \", count_class_0)\n",
        "print(\"Samples class 1: \", count_class_1)\n",
        "\n",
        "df_class_0 = df[df['label'] == 0]\n",
        "df_class_1 = df[df['label'] == 1]\n",
        "\n",
        "\n",
        "# Upsample\n",
        "df_minority_upsampled = resample(df_class_1, replace=True, n_samples=num_upsampled, random_state=42)\n",
        "\n",
        "df_upsampled = pd.concat([df_class_0, df_minority_upsampled])\n",
        "\n",
        "count_class_0, count_class_1 = df_upsampled['label'].value_counts()\n",
        "\n",
        "print(\"Samples class 0 AFTER upsample: \", count_class_0)\n",
        "print(\"Samples class 1 AFTER upsample: \", count_class_1)\n",
        "\n",
        "\n",
        "# Smote\n",
        "X_train = df_upsampled['sequence'].values\n",
        "Y_train = df_upsampled['label'].values\n",
        "\n",
        "#Convert the sequences to numeric\n",
        "numeric_train = [sequence_to_numeric(seq) for seq in X_train]\n",
        "numeric_train = np.array(numeric_train)\n",
        "\n",
        "# Reshape the data\n",
        "n_samples, _ = numeric_train.shape\n",
        "numeric_train_sequences = numeric_train.reshape((n_samples, -1))\n",
        "\n",
        "# Apply the SMOTE algorithm:\n",
        "smote = SMOTE(sampling_strategy={1: num_smote}, random_state=42)\n",
        "X_train, Y_train = smote.fit_resample(numeric_train_sequences, Y_train)\n",
        "\n",
        "#Convert the sequences to string\n",
        "X_train = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_train]\n",
        "\n",
        "data_SM = {'sequence' : X_train,\n",
        "           'label' : Y_train }\n",
        "\n",
        "df_SM = pd.DataFrame(data_SM)\n",
        "\n",
        "count_class_0, count_class_1 = df_SM['label'].value_counts()\n",
        "\n",
        "print(\"Samples class 0 AFTER upsample and SMOTE: \", count_class_0)\n",
        "print(\"Samples class 1 AFTER upsample and SMOTE: \", count_class_1)\n",
        "\n",
        "\n",
        "# OneHot encoding for the training data\n",
        "print(\"Start onehot encoding for the training data\")\n",
        "X_train = onehot_encoder(X_train)\n",
        "\n",
        "# Convert the data to a tensor\n",
        "X_train = th.from_numpy(X_train).to(device)\n",
        "Y_train = th.tensor(Y_train).to(device)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"Y_train shape: \", Y_train.shape)\n",
        "\n",
        "# Free memory\n",
        "# del train_csv, train_data, m, numeric_train, numeric_train_sequences, smote, data, df\n",
        "del train_csv, train_data, m, data, df\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "# Validation Set\n",
        "\n",
        "# Read the input from the csv file\n",
        "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
        "# Drop the NaN values\n",
        "val_csv = val_csv.dropna()\n",
        "# Describe the data\n",
        "print(val_csv.describe())\n",
        "\n",
        "val_data = val_csv.values\n",
        "# m = number of input samples\n",
        "m = val_data.shape[0]\n",
        "\n",
        "X_val = val_data[:m,1]\n",
        "Y_val = val_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the validation data\n",
        "print(\"Start onehot encoding for the validation data\")\n",
        "X_val = onehot_encoder(X_val)\n",
        "\n",
        "X_val = th.from_numpy(X_val).to(device)\n",
        "Y_val = th.tensor(Y_val).to(device)\n",
        "\n",
        "print(\"X_val shape\", X_val.shape)\n",
        "print(\"Y_val shape\", Y_val.shape)\n",
        "\n",
        "# Free memory\n",
        "del val_csv, val_data, m\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()\n",
        "\n",
        "# Test\n",
        "\n",
        "# Read the input from the csv file\n",
        "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
        "# Drop the NaN values\n",
        "test_csv = test_csv.dropna()\n",
        "# Describe the data\n",
        "print(test_csv.describe())\n",
        "\n",
        "test_data = test_csv.values\n",
        "# m = number of input samples\n",
        "m = test_data.shape[0]\n",
        "\n",
        "X_test = test_data[:m,1]\n",
        "Y_test = test_data[:m,2].astype(np.int32)\n",
        "\n",
        "# OneHot encoding for the test data\n",
        "print(\"Start onehot encoding for the test data\")\n",
        "X_test = onehot_encoder(X_test)\n",
        "\n",
        "X_test = th.from_numpy(X_test).to(device)\n",
        "Y_test = th.tensor(Y_test).to(device)\n",
        "\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"Y_test shape\", Y_test.shape)\n",
        "\n",
        "# Free memory\n",
        "del test_csv, test_data, m\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFyToHmlweav"
      },
      "source": [
        "**Weight class** and **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hd3CtvWLva1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function to calculate the weight for the two classes for\n",
        "\"\"\"\n",
        "def get_class_weights(dataset):\n",
        "    labels = [label for label in dataset]\n",
        "    class_counts = np.bincount(labels)\n",
        "    total_samples = len(labels)\n",
        "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "    return th.tensor(class_weights, dtype=th.float)\n",
        "\n",
        "# Calculate the class weights for the X_train\n",
        "class_weights = get_class_weights(Y_train.cpu()).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfGNRl3pxyO3"
      },
      "outputs": [],
      "source": [
        "# Create the Dataset\n",
        "train_dataset = CreateDataset(X_train, Y_train)\n",
        "val_dataset = CreateDataset(X_val, Y_val)\n",
        "test_dataset = CreateDataset(X_test, Y_test)\n",
        "\n",
        "# Batch size\n",
        "batch_dim = 128\n",
        "\n",
        "# Create the Data Loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_dim, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=True)\n",
        "\n",
        "# Free memory\n",
        "del X_train, X_val, Y_val\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsILBtnjmSD4"
      },
      "source": [
        "*Train* of the **CNN** using the *Validation* set to check the performance. We use *stop early* to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SC620P4mZ92",
        "outputId": "37d576da-4b54-4846-fd21-a4a9f057c2a3"
      },
      "outputs": [],
      "source": [
        "print(\"Start training the CNN\")\n",
        "\n",
        "model_CNN = ConvNet().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = th.optim.AdamW(model_CNN.parameters(), lr=0.001)\n",
        "scheduler = th.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5  # CHECK THE VALUE!!\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_CNN.train()\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "        X_batch = X_batch.float()\n",
        "        Y_batch = Y_batch.long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_CNN(X_batch.to(device))\n",
        "        loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "    # Validation the model\n",
        "    model_CNN.eval()\n",
        "    val_loss = 0.0\n",
        "    j = 0\n",
        "    with th.no_grad():\n",
        "        for X_batch, Y_batch in val_loader:\n",
        "            X_batch = X_batch.float()\n",
        "            Y_batch = Y_batch.long()\n",
        "\n",
        "            outputs = model_CNN(X_batch.to(device))\n",
        "            loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            j += 1\n",
        "\n",
        "            # Free memory\n",
        "            del X_batch, Y_batch\n",
        "            th.cuda.empty_cache()\n",
        "\n",
        "    # Losses\n",
        "    running_loss = running_loss/i\n",
        "    val_loss = val_loss/j\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss <= best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"End training the CNN\")\n",
        "\n",
        "# Free memory\n",
        "del i, j, running_loss, val_loss\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL4g50f6ocxk"
      },
      "source": [
        "**(OPTIONAL):** *Test* results for the **CNN**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBBUkJquokqI",
        "outputId": "2db8b845-d8df-4939-d2bd-3201e922e2b5"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the CNN\")\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_CNN.eval()\n",
        "\n",
        "# Arrays to save the results\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    for X_batch, Y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        outputs = model_CNN(X_batch)\n",
        "        _, predicted = th.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(Y_batch.cpu().numpy())\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true_CNN = all_labels\n",
        "y_pred_CNN = all_preds\n",
        "\n",
        "# Metrics\n",
        "accuracy_CNN = accuracy_score(y_true_CNN, y_pred_CNN)\n",
        "precision_CNN = precision_score(y_true_CNN, y_pred_CNN, average='weighted', zero_division=0)\n",
        "recall_CNN = recall_score(y_true_CNN, y_pred_CNN, average='weighted')\n",
        "f1_CNN = f1_score(y_true_CNN, y_pred_CNN, average='weighted')\n",
        "auroc_CNN = roc_auc_score(y_true_CNN, y_pred_CNN)\n",
        "\n",
        "print(f'Accuracy: {accuracy_CNN:.6f}')\n",
        "print(f'Precision: {precision_CNN:.6f}')\n",
        "print(f'Recall: {recall_CNN:.6f}')\n",
        "print(f'F1-score: {f1_CNN:.6f}')\n",
        "print(f'AUROC: {auroc_CNN:.6f}')\n",
        "\n",
        "print(\"End testing the CNN\")\n",
        "\n",
        "# Free memory\n",
        "del all_labels, all_preds, y_true_CNN, y_pred_CNN, accuracy_CNN, precision_CNN, recall_CNN, f1_CNN, auroc_CNN\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqFq1yrpMoWd",
        "outputId": "e60d2db4-f529-4b81-b048-ffe3dfe9df8c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Select the RNN.\n",
        "\"\"\"\n",
        "if type_RNN == 1:\n",
        "    print(\"GRU net selected!\")\n",
        "    model_RNN = GRU_Net().to(device)\n",
        "elif type_RNN == 2:\n",
        "    print(\"LSTML net selected!\")\n",
        "    model_RNN = LSTM_Net().to(device)\n",
        "else:\n",
        "    print(\"Not a valid net selected!\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G5i7_hGmvV7"
      },
      "source": [
        "*Train* of the **RNN** using the *Validation* set to check the performance. We use *stop early* to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ePulED-mxXd",
        "outputId": "8ae3bf49-6b7f-4032-d8e7-3ed5d13f73c4"
      },
      "outputs": [],
      "source": [
        "print(\"Start training the RNN\")\n",
        "\n",
        "# Model, loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = th.optim.Adam(model_RNN.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5  # CHECK THE VALUE!!\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_RNN.train()\n",
        "    running_loss = 0.0\n",
        "    i = 0\n",
        "    for X_batch, Y_batch in train_loader:\n",
        "\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_RNN(X_batch.to(device))\n",
        "        loss = criterion(outputs, Y_batch.to(device))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        i += 1\n",
        "\n",
        "        # Free memory\n",
        "        del X_batch, Y_batch\n",
        "        th.cuda.empty_cache()\n",
        "\n",
        "    # Validation\n",
        "    model_RNN.eval()\n",
        "    val_loss = 0.0\n",
        "    j = 0\n",
        "    with th.no_grad():\n",
        "        for X_batch, Y_batch in val_loader:\n",
        "            X_batch = X_batch.to(device).float()\n",
        "            Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "            outputs = model_RNN(X_batch)\n",
        "            loss = criterion(outputs, Y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            j += 1\n",
        "\n",
        "            # Free memory\n",
        "            del X_batch, Y_batch\n",
        "            th.cuda.empty_cache()\n",
        "\n",
        "    # Losses\n",
        "    running_loss = running_loss/i\n",
        "    val_loss = val_loss/j\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss <= best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Patience: {patience-counter}')\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "print(\"End training the RNN\")\n",
        "\n",
        "# Free memory\n",
        "del i, j, running_loss, val_loss\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCAvQnFzotd9"
      },
      "source": [
        "**(OPTIONAL):** *Test* results for the **RNN**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc4lNi6IoqON",
        "outputId": "79126391-34ce-4347-c66e-65645b3ffa78"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the RNN\")\n",
        "\n",
        "# Model ready for the evaluation\n",
        "model_RNN.eval()\n",
        "\n",
        "# Arrays to save the results\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Testing the model\n",
        "with th.no_grad():\n",
        "    for X_batch, Y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        Y_batch = Y_batch.to(device).long()\n",
        "\n",
        "        outputs = model_RNN(X_batch)\n",
        "        _, predicted = th.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(Y_batch.cpu().numpy())\n",
        "\n",
        "# Convert the tensor to use scikit learn metrics\n",
        "y_true_RNN = all_labels\n",
        "y_pred_RNN = all_preds\n",
        "\n",
        "# Metrics\n",
        "accuracy_RNN = accuracy_score(y_true_RNN, y_pred_RNN)\n",
        "precision_RNN = precision_score(y_true_RNN, y_pred_RNN, average='weighted', zero_division=0)\n",
        "recall_RNN = recall_score(y_true_RNN, y_pred_RNN, average='weighted')\n",
        "f1_RNN = f1_score(y_true_RNN, y_pred_RNN, average='weighted')\n",
        "auroc_RNN = roc_auc_score(y_true_RNN, y_pred_RNN)\n",
        "\n",
        "print(f'Accuracy: {accuracy_RNN:.6f}')\n",
        "print(f'Precision: {precision_RNN:.6f}')\n",
        "print(f'Recall: {recall_RNN:.6f}')\n",
        "print(f'F1-score: {f1_RNN:.6f}')\n",
        "print(f'AUROC: {auroc_RNN:.6f}')\n",
        "\n",
        "print(\"End testing the RNN\")\n",
        "\n",
        "# Free memory\n",
        "del all_labels, all_preds, y_true_RNN, y_pred_RNN, accuracy_RNN, precision_RNN, recall_RNN, f1_RNN, auroc_RNN\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOhwQoMemxyv"
      },
      "source": [
        "Implementation of the **Ensemble Learning** using the *stacking ensemble learing* paradigm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PlI9nhBiojA",
        "outputId": "728b4395-63dd-46a8-c139-94802678b846"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function that, given a model and a data loader, it returns all the predictions (outputs)\n",
        "\"\"\"\n",
        "def get_predictions(model_pred, data_loader):\n",
        "    model_pred.eval()\n",
        "    all_preds = []\n",
        "    with th.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            outputs = model_pred(inputs.float().to(device))\n",
        "            all_preds.append(outputs.cpu().numpy())\n",
        "    return np.concatenate(all_preds)\n",
        "\n",
        "# Prepare the prediction for the training model\n",
        "CNN_train_preds = get_predictions(model_CNN, train_loader)\n",
        "LSTM_train_preds = get_predictions(model_RNN, train_loader)\n",
        "\n",
        "# Prepare the prediction for the testing model\n",
        "CNN_test_preds = get_predictions(model_CNN, test_loader)\n",
        "LSTM_test_preds = get_predictions(model_RNN, test_loader)\n",
        "\n",
        "# Concatenate the two inputs\n",
        "X_train_meta = np.hstack((CNN_train_preds, LSTM_train_preds))\n",
        "X_test_meta = np.hstack((CNN_test_preds, LSTM_test_preds))\n",
        "\n",
        "print(\"Shape of X_train_meta: \", X_train_meta.shape)\n",
        "print(\"Shape of Y_train: \", Y_train.shape)\n",
        "\n",
        "print(\"Shape of X_test_meta: \", X_test_meta.shape)\n",
        "print(\"Shape of Y_test: \", Y_test.shape)\n",
        "\n",
        "# Define the train loader\n",
        "train_dataset = CreateDataset(X_train_meta, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
        "\n",
        "# Free memory\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBpALEtmo0G"
      },
      "source": [
        "**Test** of the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0X4pYiTm7Lm"
      },
      "outputs": [],
      "source": [
        "# We use a polynomial SVM for the final classification\n",
        "model_SVM = SVC(kernel = 'poly', probability = True)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_SVM.fit(X_batch, Y_batch.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8YnAeVfmlAC",
        "outputId": "a9a6beed-4599-41de-d42e-3caf93e40528"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the final model - SVM\")\n",
        "\n",
        "# I convert the data (if needed)\n",
        "y_true = Y_test.cpu().numpy()\n",
        "\n",
        "# Testing the meta-model: SVM\n",
        "y_pred = model_SVM.predict(X_test_meta)\n",
        "y_pred_prob = model_SVM.predict_proba(X_test_meta)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - SVM\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGnt397eqblj"
      },
      "outputs": [],
      "source": [
        "model_XGB = xgb.XGBClassifier()\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_XGB.fit(X_batch, Y_batch.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2F-yEtC6LdP",
        "outputId": "d604e3e1-7841-4206-f3e5-d3a37e5d4ab3"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the final model - XGB\")\n",
        "\n",
        "# I convert the data (if needed)\n",
        "y_true = Y_test.cpu().numpy()\n",
        "\n",
        "# Testing the meta-model: XGB\n",
        "y_pred = model_XGB.predict(X_test_meta)\n",
        "y_pred_prob = model_XGB.predict_proba(X_test_meta)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - XGB\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZnTiexFoJ6o"
      },
      "outputs": [],
      "source": [
        "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_RF.fit(X_batch, Y_batch.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_STpWxhUoh2H",
        "outputId": "dac21296-eece-461c-a7a9-ace9b388ce3f"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the final model - RF\")\n",
        "\n",
        "# I convert the data (if needed)\n",
        "y_true = Y_test.cpu().numpy()\n",
        "\n",
        "# Testing the meta-model: RF\n",
        "y_pred = model_RF.predict(X_test_meta)\n",
        "y_pred_prob = model_RF.predict_proba(X_test_meta)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - RF\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwz10Gf4qGOH"
      },
      "outputs": [],
      "source": [
        "model_GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "for X_batch, Y_batch in train_loader:\n",
        "    X_batch = X_batch.cpu()\n",
        "    Y_batch = Y_batch.cpu()\n",
        "    model_GBC.fit(X_batch, Y_batch.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhNFT4ThqWAn",
        "outputId": "f7618848-f780-4918-e882-02267c02ecfa"
      },
      "outputs": [],
      "source": [
        "print(\"Start testing the final model - GBC\")\n",
        "\n",
        "# I convert the data (if needed)\n",
        "y_true = Y_test.cpu().numpy()\n",
        "\n",
        "# Testing the meta-model: GBC\n",
        "y_pred = model_GBC.predict(X_test_meta)\n",
        "y_pred_prob = model_GBC.predict_proba(X_test_meta)[:, 1]\n",
        "\n",
        "# Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "auroc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.6f}')\n",
        "print(f'Precision: {precision:.6f}')\n",
        "print(f'Recall: {recall:.6f}')\n",
        "print(f'F1-score: {f1:.6f}')\n",
        "print(f'AUROC: {auroc:.6f}')\n",
        "\n",
        "print(\"End testing the final model - GBC\")\n",
        "\n",
        "# Free memory\n",
        "del y_pred, y_pred_prob, y_true, accuracy, precision, recall, f1, auroc\n",
        "gc.collect()\n",
        "th.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
