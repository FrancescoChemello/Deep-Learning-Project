{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main libraries\n",
    "\"\"\"\n",
    "import os\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for the onehot encoding\n",
    "\"\"\"\n",
    "\n",
    "def onehot_encoder(dataset):\n",
    "    \"\"\"\n",
    "    Function that encodes a DNA dataset into a onehot encoding dataset.\n",
    "    \"\"\"\n",
    "    onehot_dataset = [dna_onehot_encoder(dna_string) for dna_string in dataset]\n",
    "    onehot_dataset_numpy = np.array(onehot_dataset)\n",
    "\n",
    "    return onehot_dataset_numpy\n",
    "\n",
    "\n",
    "def dna_onehot_encoder(dna_sequence):\n",
    "    \"\"\"\n",
    "    Function that encodes a single DNA string into a onehot encoding string.\n",
    "    \"\"\"\n",
    "    onehot_dict = {\n",
    "        'A' : [1, 0, 0, 0],\n",
    "        'C' : [0, 1, 0, 0],\n",
    "        'G' : [0, 0, 1, 0],\n",
    "        'T' : [0, 0, 0, 1]\n",
    "    }\n",
    "    encoder = [onehot_dict[nuc] for nuc in dna_sequence]\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Net (nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(GRU_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.gru = nn.GRU(input_size=128, hidden_size=128, num_layers=6, batch_first=True, dropout=0.5, bidirectional=True)\n",
    "        # self.GRU = nn.GRU(input_size=4, hidden_size=128, num_layers=2, dropout=0.5, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.GRU(x)\n",
    "\n",
    "        x = x[:, -1, :]\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_numeric(sequence):\n",
    "    mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "    return [mapping[nuc] for nuc in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Start one hot encoding for training\n",
      "End one hot encoding for training\n",
      "X_train shape:  torch.Size([413544, 300, 4])\n",
      "Y_train shape:  torch.Size([413544])\n",
      "Start one hot encoding for validation\n",
      "End one hot encoding for validation\n",
      "X_val shape:  torch.Size([26404, 300, 4])\n",
      "Y_val shape:  torch.Size([26404])\n",
      "Start one hot encoding for test\n",
      "End one hot encoding for test\n",
      "X_test shape:  torch.Size([26404, 300, 4])\n",
      "Y_test shape:  torch.Size([26404])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "! MAIN\n",
    "\"\"\"\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(), 'data'))\n",
    "rel_path_train = os.path.join(data_dir, 'fullset_train.csv')\n",
    "rel_path_val = os.path.join(data_dir, 'fullset_validation.csv')\n",
    "rel_path_test = os.path.join(data_dir, 'fullset_test.csv')\n",
    "\n",
    "# Training set\n",
    "\n",
    "# Read the input from the csv file\n",
    "train_csv = pd.read_csv(rel_path_train, sep=\",\")\n",
    "# Drop the NaN values\n",
    "train_csv = train_csv.dropna()\n",
    "# Describe the data\n",
    "# print(train_csv.describe())\n",
    "\n",
    "# Get the data from the csv file\n",
    "train_data = train_csv.values\n",
    "# m = number of input samples\n",
    "m = train_data.shape[0]\n",
    "\n",
    "# Dataframe\n",
    "data = {'sequence' : train_data[:m,1],\n",
    "        'label' : train_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_train = df['sequence'].values\n",
    "y_train = df['label'].values\n",
    "\n",
    "#Convert the sequences to numeric\n",
    "numeric_train = [sequence_to_numeric(seq) for seq in X_train]\n",
    "numeric_train = np.array(numeric_train)\n",
    "\n",
    "# Reshape the data\n",
    "n_samples, _ = numeric_train.shape\n",
    "numeric_train_sequences = numeric_train.reshape((n_samples, -1))\n",
    "\n",
    "# Apply the SMOTE algorithm to balance the dataset\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_resample(numeric_train_sequences, y_train)\n",
    "\n",
    "#Convert the sequences to string\n",
    "X_train = [''.join([['A', 'C', 'G', 'T'][nuc] for nuc in seq]) for seq in X_train]\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for training\")\n",
    "X_train = onehot_encoder(X_train)\n",
    "print(\"End one hot encoding for training\")\n",
    "\n",
    "X_train = th.from_numpy(X_train).to(device)\n",
    "Y_train = th.tensor(y_train).to(device)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "\n",
    "# Validation set\n",
    "# Read the input from the csv file\n",
    "val_csv = pd.read_csv(rel_path_val, sep=\",\")\n",
    "# Drop the NaN values\n",
    "val_csv = val_csv.dropna()\n",
    "\n",
    "val_data = val_csv.values\n",
    "# m = number of input samples\n",
    "m = val_data.shape[0]\n",
    "\n",
    "# Dataframe and upsample\n",
    "data = {'sequence' : val_data[:m,1],\n",
    "        'label' : val_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_val = df['sequence'].values\n",
    "y_val = df['label'].values\n",
    "\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for validation\")\n",
    "X_val = onehot_encoder(X_val)\n",
    "print(\"End one hot encoding for validation\")\n",
    "\n",
    "X_val = th.from_numpy(X_val).to(device)\n",
    "Y_val = th.tensor(y_val).to(device)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"Y_val shape: \", Y_val.shape)\n",
    "\n",
    "# Test set\n",
    "# Read the input from the csv file\n",
    "test_csv = pd.read_csv(rel_path_test, sep=\",\")\n",
    "# Drop the NaN values\n",
    "test_csv = test_csv.dropna()\n",
    "\n",
    "test_data = test_csv.values\n",
    "# m = number of input samples\n",
    "m = test_data.shape[0]\n",
    "\n",
    "# Dataframe and upsample\n",
    "data = {'sequence' : test_data[:m,1],\n",
    "        'label' : test_data[:m,2].astype(np.int32) }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_test = df['sequence'].values\n",
    "y_test = df['label'].values\n",
    "\n",
    "# Convert the sequences to onehot encoding\n",
    "print(\"Start one hot encoding for test\")\n",
    "X_test = onehot_encoder(X_test)\n",
    "print(\"End one hot encoding for test\")\n",
    "\n",
    "X_test = th.from_numpy(X_test).to(device)\n",
    "Y_test = th.tensor(y_test).to(device)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)\n",
    "\n",
    "#free memory\n",
    "del train_csv, val_csv, test_csv, train_data, val_data, test_data, data, df, numeric_train, numeric_train_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate the weight for the two classes for\n",
    "\"\"\"\n",
    "\n",
    "Y_train_cpu = Y_train.cpu().numpy()\n",
    "def get_class_weights(dataset):\n",
    "    labels = [label for label in dataset]\n",
    "    class_counts = np.bincount(labels)\n",
    "    total_samples = len(labels)\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    return th.tensor(class_weights, dtype=th.float)\n",
    "\n",
    "# Calculate the class weights for the X_train\n",
    "class_weights = get_class_weights(Y_train_cpu).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create a Data Loader with X label and Y label together\n",
    "\"\"\"\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return data_point, label\n",
    "\n",
    "# Create the Dataset\n",
    "train_dataset = CreateDataset(X_train, Y_train)\n",
    "val_dataset = CreateDataset(X_val, Y_val)\n",
    "test_dataset = CreateDataset(X_test, Y_test)\n",
    "\n",
    "# Batch size\n",
    "batch_dim = 128\n",
    "\n",
    "# Create the Data Loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_dim, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_dim, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_dim, shuffle=True)\n",
    "\n",
    "# Free memory\n",
    "del X_train, X_val, Y_val, X_test\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training the GRU\")\n",
    "\n",
    "# Model, loss function and optimizer\n",
    "model_GRU = GRU_Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = th.optim.Adam(model_GRU.parameters(), lr=0.001)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # CHECK THE VALUE!!\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100  # Hope to reach convergence before 100 epoches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_GRU.train()\n",
    "    running_loss = 0.0\n",
    "    i = 0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "\n",
    "        X_batch = X_batch.float().to(device)\n",
    "        Y_batch = Y_batch.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_GRU(X_batch.to(device))\n",
    "        loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "\n",
    "        \"\"\"\n",
    "        scaler = th.cuda.amp.GradScaler()\n",
    "        with th.cuda.amp.autocast():\n",
    "          outputs = model_GRU(X_batch.to(device))\n",
    "          loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\"\"\"\n",
    "\n",
    "        # Free memory\n",
    "        del X_batch, Y_batch\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "    # Validation\n",
    "    model_GRU.eval()\n",
    "    val_loss = 0.0\n",
    "    j = 0\n",
    "    with th.no_grad():\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch = X_batch.float()\n",
    "            Y_batch = Y_batch.long()\n",
    "\n",
    "            outputs = model_GRU(X_batch.to(device))\n",
    "            loss = criterion(outputs, Y_batch.to(device))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            j += 1\n",
    "\n",
    "            # Free memory\n",
    "            del X_batch, Y_batch\n",
    "            th.cuda.empty_cache()\n",
    "\n",
    "    # Losses\n",
    "    running_loss = running_loss/i\n",
    "    val_loss = val_loss/j\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss <= best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"End training the GRU\")\n",
    "\n",
    "# Free memory\n",
    "del i, j, running_loss, val_loss\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing the LSTM\n",
      "Accuracy: 0.167918\n",
      "Precision: 0.028196\n",
      "Recall: 0.167918\n",
      "F1-score: 0.048285\n",
      "End testing the LSTM\n"
     ]
    }
   ],
   "source": [
    "print(\"Start testing the GRU\")\n",
    "\n",
    "# Model ready for the evaluation\n",
    "model_GRU.eval()\n",
    "\n",
    "# Arrays to save the results\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Testing the model\n",
    "with th.no_grad():\n",
    "    for X_batch, Y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device).float()\n",
    "        Y_batch = Y_batch.to(device).long()\n",
    "\n",
    "        outputs = model_GRU(X_batch)\n",
    "        _, predicted = th.max(outputs, 1)\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(Y_batch.cpu().numpy())\n",
    "\n",
    "# Convert the tensor to use scikit learn metrics\n",
    "y_true_GRU = all_labels\n",
    "y_pred_GRU = all_preds\n",
    "\n",
    "# Metrics\n",
    "accuracy_GRU = accuracy_score(y_true_GRU, y_pred_GRU)\n",
    "precision_GRU = precision_score(y_true_GRU, y_pred_GRU, average='weighted', zero_division=0)\n",
    "recall_GRU = recall_score(y_true_GRU, y_pred_GRU, average='weighted')\n",
    "f1_GRU = f1_score(y_true_GRU, y_pred_GRU, average='weighted')\n",
    "auroc_GRU = roc_auc_score(y_true_GRU, y_pred_GRU)\n",
    "\n",
    "print(f'Accuracy: {accuracy_GRU:.6f}')\n",
    "print(f'Precision: {precision_GRU:.6f}')\n",
    "print(f'Recall: {recall_GRU:.6f}')\n",
    "print(f'F1-score: {f1_GRU:.6f}')\n",
    "print(f'AUROC: {auroc_GRU:.6f}')\n",
    "\n",
    "print(\"End testing the GRU\")\n",
    "\n",
    "# Free memory\n",
    "del all_labels, all_preds, y_true_GRU, y_pred_GRU, accuracy_GRU, precision_GRU, recall_GRU, f1_GRU\n",
    "gc.collect()\n",
    "th.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
